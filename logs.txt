
==> Audit <==
|---------|-------------------|----------|-------------------------|---------|---------------------|---------------------|
| Command |       Args        | Profile  |          User           | Version |     Start Time      |      End Time       |
|---------|-------------------|----------|-------------------------|---------|---------------------|---------------------|
| start   | --driver=docker   | minikube | DESKTOP-0PI4HTS\CSELAB2 | v1.35.0 | 12 Mar 24 09:30 IST |                     |
| start   | --driver=docker   | minikube | DESKTOP-0PI4HTS\CSELAB2 | v1.35.0 | 12 Mar 25 09:41 IST | 12 Mar 25 09:43 IST |
| image   | load registration | minikube | DESKTOP-0PI4HTS\CSELAB2 | v1.35.0 | 12 Mar 25 09:44 IST |                     |
| image   | load registration | minikube | DESKTOP-0PI4HTS\CSELAB2 | v1.35.0 | 12 Mar 25 09:48 IST |                     |
|---------|-------------------|----------|-------------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/03/12 09:41:17
Running on machine: DESKTOP-0PI4HTS
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0312 09:41:17.126555   21168 out.go:345] Setting OutFile to fd 112 ...
I0312 09:41:17.127138   21168 out.go:397] isatty.IsTerminal(112) = true
I0312 09:41:17.127138   21168 out.go:358] Setting ErrFile to fd 116...
I0312 09:41:17.127138   21168 out.go:397] isatty.IsTerminal(116) = true
W0312 09:41:17.135758   21168 root.go:314] Error reading config file at C:\Users\CSELAB2\.minikube\config\config.json: open C:\Users\CSELAB2\.minikube\config\config.json: The system cannot find the file specified.
I0312 09:41:17.147585   21168 out.go:352] Setting JSON to false
I0312 09:41:17.155076   21168 start.go:129] hostinfo: {"hostname":"DESKTOP-0PI4HTS","uptime":2621,"bootTime":1741750055,"procs":243,"os":"windows","platform":"Microsoft Windows 11 Pro","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.3194 Build 26100.3194","kernelVersion":"10.0.26100.3194 Build 26100.3194","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"d32281f2-6151-4b05-867b-d4f6da54bc5b"}
W0312 09:41:17.155076   21168 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0312 09:41:17.155581   21168 out.go:177] 😄  minikube v1.35.0 on Microsoft Windows 11 Pro 10.0.26100.3194 Build 26100.3194
I0312 09:41:17.156618   21168 notify.go:220] Checking for updates...
W0312 09:41:17.156618   21168 preload.go:293] Failed to list preload files: open C:\Users\CSELAB2\.minikube\cache\preloaded-tarball: The system cannot find the file specified.
I0312 09:41:17.169454   21168 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0312 09:41:17.178133   21168 driver.go:394] Setting default libvirt URI to qemu:///system
I0312 09:41:17.215572   21168 docker.go:123] docker version: linux-27.5.1:Docker Desktop 4.38.0 (181591)
I0312 09:41:17.216635   21168 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0312 09:41:18.108575   21168 info.go:266] docker info: {ID:94856518-f9bd-4125-9bb9-e94a96ef5120 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:59 OomKillDisable:true NGoroutines:85 SystemTime:2025-03-12 04:11:18.099018918 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:20 MemTotal:8166682624 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-buildx.exe] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-debug.exe] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-desktop.exe] ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-dev.exe] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-extension.exe] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-feedback.exe] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-init.exe] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-sbom.exe] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0312 09:41:18.111829   21168 out.go:177] ✨  Using the docker driver based on existing profile
I0312 09:41:18.112358   21168 start.go:297] selected driver: docker
I0312 09:41:18.112358   21168 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\CSELAB2:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0312 09:41:18.112358   21168 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0312 09:41:18.114513   21168 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0312 09:41:18.230108   21168 info.go:266] docker info: {ID:94856518-f9bd-4125-9bb9-e94a96ef5120 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:1 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:59 OomKillDisable:true NGoroutines:85 SystemTime:2025-03-12 04:11:18.220910611 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:20 MemTotal:8166682624 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-buildx.exe] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-debug.exe] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-desktop.exe] ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-dev.exe] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-extension.exe] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-feedback.exe] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-init.exe] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-sbom.exe] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0312 09:41:18.258288   21168 cni.go:84] Creating CNI manager for ""
I0312 09:41:18.258288   21168 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0312 09:41:18.258288   21168 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP: Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\CSELAB2:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0312 09:41:18.259921   21168 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0312 09:41:18.260968   21168 cache.go:121] Beginning downloading kic base image for docker with docker
I0312 09:41:18.262049   21168 out.go:177] 🚜  Pulling base image v0.0.46 ...
I0312 09:41:18.262565   21168 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0312 09:41:18.263087   21168 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0312 09:41:18.320701   21168 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0312 09:41:18.321244   21168 localpath.go:146] windows sanitize: C:\Users\CSELAB2\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\CSELAB2\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0312 09:41:18.321244   21168 localpath.go:146] windows sanitize: C:\Users\CSELAB2\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\CSELAB2\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0312 09:41:18.321244   21168 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0312 09:41:18.321244   21168 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0312 09:41:18.534678   21168 preload.go:118] Found remote preload: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0312 09:41:18.534678   21168 cache.go:56] Caching tarball of preloaded images
I0312 09:41:18.534678   21168 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0312 09:41:18.536224   21168 out.go:177] 💾  Downloading Kubernetes v1.32.0 preload ...
I0312 09:41:18.537278   21168 preload.go:236] getting checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0312 09:41:18.930929   21168 download.go:108] Downloading: https://storage.googleapis.com/minikube-preloaded-volume-tarballs/v18/v1.32.0/preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4?checksum=md5:4da2ed9bc13e09e8e9b7cf53d01335db -> C:\Users\CSELAB2\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0312 09:41:39.402831   21168 cache.go:169] failed to download gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279, will try fallback image if available: getting remote image: Get "https://gcr.io/v2/": dial tcp 192.0.2.1:443: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.
I0312 09:41:39.402831   21168 image.go:81] Checking for docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0312 09:41:39.455990   21168 cache.go:150] Downloading docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0312 09:41:39.455990   21168 localpath.go:146] windows sanitize: C:\Users\CSELAB2\.minikube\cache\kic\amd64\stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\CSELAB2\.minikube\cache\kic\amd64\stable_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0312 09:41:39.456515   21168 localpath.go:146] windows sanitize: C:\Users\CSELAB2\.minikube\cache\kic\amd64\stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\CSELAB2\.minikube\cache\kic\amd64\stable_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0312 09:41:39.456515   21168 image.go:65] Checking for docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0312 09:41:39.456515   21168 image.go:150] Writing docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0312 09:41:42.122102   21168 preload.go:247] saving checksum for preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0312 09:41:42.123096   21168 preload.go:254] verifying checksum of C:\Users\CSELAB2\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 ...
I0312 09:41:42.600607   21168 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0312 09:41:42.600607   21168 profile.go:143] Saving config to C:\Users\CSELAB2\.minikube\profiles\minikube\config.json ...
I0312 09:42:00.634765   21168 cache.go:169] failed to download docker.io/kicbase/stable:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279, will try fallback image if available: getting remote image: Get "https://index.docker.io/v2/": dial tcp 192.0.2.1:443: connectex: A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond.
I0312 09:42:00.634765   21168 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46 in local docker daemon
I0312 09:42:00.692491   21168 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46 to local cache
I0312 09:42:00.692491   21168 localpath.go:146] windows sanitize: C:\Users\CSELAB2\.minikube\cache\kic\amd64\kicbase:v0.0.46.tar -> C:\Users\CSELAB2\.minikube\cache\kic\amd64\kicbase_v0.0.46.tar
I0312 09:42:00.692491   21168 localpath.go:146] windows sanitize: C:\Users\CSELAB2\.minikube\cache\kic\amd64\kicbase:v0.0.46.tar -> C:\Users\CSELAB2\.minikube\cache\kic\amd64\kicbase_v0.0.46.tar
I0312 09:42:00.693012   21168 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46 in local cache directory
I0312 09:42:00.693012   21168 image.go:150] Writing gcr.io/k8s-minikube/kicbase:v0.0.46 to local cache
I0312 09:42:35.306942   21168 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46 as a tarball
I0312 09:42:35.306942   21168 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46 from local cache
I0312 09:42:35.306942   21168 localpath.go:146] windows sanitize: C:\Users\CSELAB2\.minikube\cache\kic\amd64\kicbase:v0.0.46.tar -> C:\Users\CSELAB2\.minikube\cache\kic\amd64\kicbase_v0.0.46.tar
I0312 09:42:54.354196   21168 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46 from cached tarball
I0312 09:42:54.354713   21168 cache.go:227] Successfully downloaded all kic artifacts
I0312 09:42:54.355767   21168 start.go:360] acquireMachinesLock for minikube: {Name:mk4c39923d40c63f62e1f70e4b0dc20a3db59b7e Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0312 09:42:54.355767   21168 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0312 09:42:54.356308   21168 start.go:96] Skipping create...Using existing machine configuration
I0312 09:42:54.356308   21168 fix.go:54] fixHost starting: 
I0312 09:42:54.359486   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0312 09:42:54.391898   21168 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0312 09:42:54.391898   21168 fix.go:112] recreateIfNeeded on minikube: state= err=unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:42:54.391898   21168 fix.go:117] machineExists: false. err=machine does not exist
I0312 09:42:54.392437   21168 out.go:177] 🤷  docker "minikube" container is missing, will recreate.
I0312 09:42:54.393524   21168 delete.go:124] DEMOLISHING minikube ...
I0312 09:42:54.395133   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0312 09:42:54.424046   21168 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0312 09:42:54.424576   21168 stop.go:83] unable to get state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:42:54.424576   21168 delete.go:128] stophost failed (probably ok): ssh power off: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:42:54.428170   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0312 09:42:54.454491   21168 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0312 09:42:54.454491   21168 delete.go:82] Unable to get host status for minikube, assuming it has already been deleted: state: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:42:54.455565   21168 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0312 09:42:54.482007   21168 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0312 09:42:54.482007   21168 kic.go:371] could not find the container minikube to remove it. will try anyways
I0312 09:42:54.483058   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0312 09:42:54.509740   21168 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
W0312 09:42:54.510300   21168 oci.go:84] error getting container status, will try to delete anyways: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:42:54.511339   21168 cli_runner.go:164] Run: docker exec --privileged -t minikube /bin/bash -c "sudo init 0"
W0312 09:42:54.537802   21168 cli_runner.go:211] docker exec --privileged -t minikube /bin/bash -c "sudo init 0" returned with exit code 1
I0312 09:42:54.538351   21168 oci.go:656] error shutdown minikube: docker exec --privileged -t minikube /bin/bash -c "sudo init 0": exit status 1
stdout:

stderr:
Error response from daemon: No such container: minikube
I0312 09:42:55.542671   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0312 09:42:55.578936   21168 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0312 09:42:55.578936   21168 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:42:55.578936   21168 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0312 09:42:55.578936   21168 retry.go:31] will retry after 432.030853ms: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:42:56.014173   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0312 09:42:56.045538   21168 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0312 09:42:56.045538   21168 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:42:56.045538   21168 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0312 09:42:56.045538   21168 retry.go:31] will retry after 826.394218ms: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:42:56.873918   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0312 09:42:56.923033   21168 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0312 09:42:56.923033   21168 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:42:56.923033   21168 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0312 09:42:56.923033   21168 retry.go:31] will retry after 865.927958ms: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:42:57.793541   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0312 09:42:57.826197   21168 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0312 09:42:57.826197   21168 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:42:57.826197   21168 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0312 09:42:57.826197   21168 retry.go:31] will retry after 2.199799232s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:43:00.030069   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0312 09:43:00.066679   21168 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0312 09:43:00.066679   21168 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:43:00.066679   21168 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0312 09:43:00.066679   21168 retry.go:31] will retry after 1.978452514s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:43:02.048783   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0312 09:43:02.082449   21168 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0312 09:43:02.082449   21168 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:43:02.082449   21168 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0312 09:43:02.082449   21168 retry.go:31] will retry after 5.177009907s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:43:07.261969   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0312 09:43:07.292734   21168 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0312 09:43:07.292734   21168 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:43:07.292734   21168 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0312 09:43:07.293386   21168 retry.go:31] will retry after 3.146528381s: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:43:10.442745   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0312 09:43:10.497101   21168 cli_runner.go:211] docker container inspect minikube --format={{.State.Status}} returned with exit code 1
I0312 09:43:10.497101   21168 oci.go:668] temporary error verifying shutdown: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
I0312 09:43:10.497101   21168 oci.go:670] temporary error: container minikube status is  but expect it to be exited
I0312 09:43:10.497101   21168 oci.go:88] couldn't shut down minikube (might be okay): verify shutdown: couldn't verify container is exited. %v: unknown state "minikube": docker container inspect minikube --format={{.State.Status}}: exit status 1
stdout:


stderr:
Error response from daemon: No such container: minikube
 
I0312 09:43:10.498227   21168 cli_runner.go:164] Run: docker rm -f -v minikube
I0312 09:43:10.527039   21168 cli_runner.go:164] Run: docker container inspect -f {{.Id}} minikube
W0312 09:43:10.555367   21168 cli_runner.go:211] docker container inspect -f {{.Id}} minikube returned with exit code 1
I0312 09:43:10.556408   21168 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0312 09:43:10.585953   21168 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0312 09:43:10.587016   21168 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0312 09:43:10.587016   21168 cli_runner.go:164] Run: docker network inspect minikube
W0312 09:43:10.615700   21168 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0312 09:43:10.615700   21168 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0312 09:43:10.615700   21168 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0312 09:43:10.616791   21168 fix.go:124] Sleeping 1 second for extra luck!
I0312 09:43:11.617316   21168 start.go:125] createHost starting for "" (driver="docker")
I0312 09:43:11.636723   21168 out.go:235] 🔥  Creating docker container (CPUs=2, Memory=4000MB) ...
I0312 09:43:11.637321   21168 start.go:159] libmachine.API.Create for "minikube" (driver="docker")
I0312 09:43:11.637321   21168 client.go:168] LocalClient.Create starting
I0312 09:43:11.638359   21168 main.go:141] libmachine: Reading certificate data from C:\Users\CSELAB2\.minikube\certs\ca.pem
I0312 09:43:11.647345   21168 main.go:141] libmachine: Decoding PEM data...
I0312 09:43:11.647345   21168 main.go:141] libmachine: Parsing certificate...
I0312 09:43:11.648454   21168 main.go:141] libmachine: Reading certificate data from C:\Users\CSELAB2\.minikube\certs\cert.pem
I0312 09:43:11.658001   21168 main.go:141] libmachine: Decoding PEM data...
I0312 09:43:11.658001   21168 main.go:141] libmachine: Parsing certificate...
I0312 09:43:11.660154   21168 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
W0312 09:43:11.690695   21168 cli_runner.go:211] docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}" returned with exit code 1
I0312 09:43:11.691746   21168 network_create.go:284] running [docker network inspect minikube] to gather additional debugging logs...
I0312 09:43:11.691746   21168 cli_runner.go:164] Run: docker network inspect minikube
W0312 09:43:11.719526   21168 cli_runner.go:211] docker network inspect minikube returned with exit code 1
I0312 09:43:11.719526   21168 network_create.go:287] error running [docker network inspect minikube]: docker network inspect minikube: exit status 1
stdout:
[]

stderr:
Error response from daemon: network minikube not found
I0312 09:43:11.719526   21168 network_create.go:289] output of [docker network inspect minikube]: -- stdout --
[]

-- /stdout --
** stderr ** 
Error response from daemon: network minikube not found

** /stderr **
I0312 09:43:11.720628   21168 cli_runner.go:164] Run: docker network inspect bridge --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0312 09:43:11.752453   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.755574   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.758188   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.760819   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.763970   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.766581   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.769206   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.772324   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.774922   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.777993   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.781004   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.784060   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.787573   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.790793   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.793888   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.796523   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.799110   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.802792   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.805960   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
I0312 09:43:11.808598   21168 network.go:211] skipping subnet 192.168.0.0/16 that is taken: &{IP:192.168.0.0 Netmask:255.255.0.0 Prefix:16 CIDR:192.168.0.0/16 Gateway:192.168.21.82 ClientMin:192.168.21.83 ClientMax:192.168.255.254 Broadcast:192.168.255.255 IsPrivate:true Interface:{IfaceName:Ethernet IfaceIPv4:192.168.21.82 IfaceMTU:1500 IfaceMAC:4c:d7:17:80:25:95} reservation:<nil>}
E0312 09:43:11.808598   21168 network_create.go:103] failed to find free subnet for docker network minikube after 20 attempts: no free private network subnets found with given parameters (start: "192.168.49.0", step: 9, tries: 20)
W0312 09:43:11.809652   21168 out.go:270] ❗  Unable to create dedicated network, this might result in cluster IP change after restart: un-retryable: no free private network subnets found with given parameters (start: "192.168.49.0", step: 9, tries: 20)
I0312 09:43:11.811756   21168 cli_runner.go:164] Run: docker ps -a --format {{.Names}}
I0312 09:43:11.850839   21168 cli_runner.go:164] Run: docker volume create minikube --label name.minikube.sigs.k8s.io=minikube --label created_by.minikube.sigs.k8s.io=true
I0312 09:43:11.881724   21168 oci.go:103] Successfully created a docker volume minikube
I0312 09:43:11.882789   21168 cli_runner.go:164] Run: docker run --rm --name minikube-preload-sidecar --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --entrypoint /usr/bin/test -v minikube:/var gcr.io/k8s-minikube/kicbase:v0.0.46 -d /var/lib
I0312 09:43:12.665930   21168 oci.go:107] Successfully prepared a docker volume minikube
I0312 09:43:12.665930   21168 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0312 09:43:12.665930   21168 kic.go:194] Starting extracting preloaded images to volume ...
I0312 09:43:12.666979   21168 cli_runner.go:164] Run: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\CSELAB2\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46 -I lz4 -xf /preloaded.tar -C /extractDir
I0312 09:43:25.941001   21168 cli_runner.go:217] Completed: docker run --rm --entrypoint /usr/bin/tar -v C:\Users\CSELAB2\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4:/preloaded.tar:ro -v minikube:/extractDir gcr.io/k8s-minikube/kicbase:v0.0.46 -I lz4 -xf /preloaded.tar -C /extractDir: (13.2740208s)
I0312 09:43:25.941001   21168 kic.go:203] duration metric: took 13.2750697s to extract preloaded images to volume ...
I0312 09:43:25.943873   21168 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0312 09:43:26.074192   21168 info.go:266] docker info: {ID:94856518-f9bd-4125-9bb9-e94a96ef5120 Containers:1 ContainersRunning:0 ContainersPaused:0 ContainersStopped:1 Images:3 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:52 OomKillDisable:true NGoroutines:80 SystemTime:2025-03-12 04:13:26.062803877 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:12 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:20 MemTotal:8166682624 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:27.5.1 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb Expected:bcc810d6b9066471b0b6fa75f557a15a1cbf31bb} RuncCommit:{ID:v1.1.12-0-g51d5e946 Expected:v1.1.12-0-g51d5e946} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-ai.exe] ShortDescription:Ask Gordon - Docker Agent Vendor:Docker Inc. Version:v0.7.3] map[Name:buildx Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-buildx.exe] ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.20.1-desktop.2] map[Name:compose Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-compose.exe] ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.32.4-desktop.1] map[Name:debug Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-debug.exe] ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-desktop.exe] ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.4] map[Name:dev Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-dev.exe] ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-extension.exe] ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:feedback Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-feedback.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-feedback.exe] ShortDescription:Provide feedback, right in your terminal! Vendor:Docker Inc. Version:v1.0.5] map[Name:init Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-init.exe] ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-sbom.exe] ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Users\CSELAB2\.docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShadowedPaths:[C:\Program Files\Docker\cli-plugins\docker-scout.exe] ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.16.1]] Warnings:<nil>}}
I0312 09:43:26.075767   21168 cli_runner.go:164] Run: docker info --format "'{{json .SecurityOptions}}'"
I0312 09:43:26.208558   21168 cli_runner.go:164] Run: docker run -d -t --privileged --security-opt seccomp=unconfined --tmpfs /tmp --tmpfs /run -v /lib/modules:/lib/modules:ro --hostname minikube --name minikube --label created_by.minikube.sigs.k8s.io=true --label name.minikube.sigs.k8s.io=minikube --label role.minikube.sigs.k8s.io= --label mode.minikube.sigs.k8s.io=minikube --volume minikube:/var --security-opt apparmor=unconfined --memory=4000mb --memory-swap=4000mb --cpus=2 -e container=docker --expose 8443 --publish=127.0.0.1::8443 --publish=127.0.0.1::22 --publish=127.0.0.1::2376 --publish=127.0.0.1::5000 --publish=127.0.0.1::32443 gcr.io/k8s-minikube/kicbase:v0.0.46
I0312 09:43:26.613450   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Running}}
I0312 09:43:26.649102   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0312 09:43:26.685822   21168 cli_runner.go:164] Run: docker exec minikube stat /var/lib/dpkg/alternatives/iptables
I0312 09:43:26.752440   21168 oci.go:144] the created container "minikube" has a running status.
I0312 09:43:26.752964   21168 kic.go:225] Creating ssh key for kic: C:\Users\CSELAB2\.minikube\machines\minikube\id_rsa...
I0312 09:43:26.871574   21168 kic_runner.go:191] docker (temp): C:\Users\CSELAB2\.minikube\machines\minikube\id_rsa.pub --> /home/docker/.ssh/authorized_keys (381 bytes)
I0312 09:43:26.921624   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0312 09:43:26.962416   21168 kic_runner.go:93] Run: chown docker:docker /home/docker/.ssh/authorized_keys
I0312 09:43:26.962416   21168 kic_runner.go:114] Args: [docker exec --privileged minikube chown docker:docker /home/docker/.ssh/authorized_keys]
I0312 09:43:27.031235   21168 kic.go:265] ensuring only current user has permissions to key file located at : C:\Users\CSELAB2\.minikube\machines\minikube\id_rsa...
W0312 09:43:27.031235   21168 kic.go:271] unable to determine current user's SID. minikube tunnel may not work.
I0312 09:43:27.033336   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0312 09:43:27.066063   21168 machine.go:93] provisionDockerMachine start ...
I0312 09:43:27.067100   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:27.102035   21168 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0312 09:43:27.110515   21168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa05360] 0xa07ea0 <nil>  [] 0s} 127.0.0.1 57368 <nil> <nil>}
I0312 09:43:27.110515   21168 main.go:141] libmachine: About to run SSH command:
hostname
I0312 09:43:27.237412   21168 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0312 09:43:27.238015   21168 ubuntu.go:169] provisioning hostname "minikube"
I0312 09:43:27.239566   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:27.269406   21168 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0312 09:43:27.269941   21168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa05360] 0xa07ea0 <nil>  [] 0s} 127.0.0.1 57368 <nil> <nil>}
I0312 09:43:27.269941   21168 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0312 09:43:27.405862   21168 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0312 09:43:27.407583   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:27.437956   21168 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0312 09:43:27.437956   21168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa05360] 0xa07ea0 <nil>  [] 0s} 127.0.0.1 57368 <nil> <nil>}
I0312 09:43:27.437956   21168 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0312 09:43:27.571664   21168 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0312 09:43:27.571664   21168 ubuntu.go:175] set auth options {CertDir:C:\Users\CSELAB2\.minikube CaCertPath:C:\Users\CSELAB2\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\CSELAB2\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\CSELAB2\.minikube\machines\server.pem ServerKeyPath:C:\Users\CSELAB2\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\CSELAB2\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\CSELAB2\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\CSELAB2\.minikube}
I0312 09:43:27.571664   21168 ubuntu.go:177] setting up certificates
I0312 09:43:27.571664   21168 provision.go:84] configureAuth start
I0312 09:43:27.572793   21168 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0312 09:43:27.603537   21168 provision.go:143] copyHostCerts
I0312 09:43:27.603537   21168 exec_runner.go:151] cp: C:\Users\CSELAB2\.minikube\certs\ca.pem --> C:\Users\CSELAB2\.minikube/ca.pem (1078 bytes)
I0312 09:43:27.604074   21168 exec_runner.go:151] cp: C:\Users\CSELAB2\.minikube\certs\cert.pem --> C:\Users\CSELAB2\.minikube/cert.pem (1123 bytes)
I0312 09:43:27.612657   21168 exec_runner.go:151] cp: C:\Users\CSELAB2\.minikube\certs\key.pem --> C:\Users\CSELAB2\.minikube/key.pem (1675 bytes)
I0312 09:43:27.612657   21168 provision.go:117] generating server cert: C:\Users\CSELAB2\.minikube\machines\server.pem ca-key=C:\Users\CSELAB2\.minikube\certs\ca.pem private-key=C:\Users\CSELAB2\.minikube\certs\ca-key.pem org=CSELAB2.minikube san=[127.0.0.1 172.17.0.2 localhost minikube]
I0312 09:43:27.776246   21168 provision.go:177] copyRemoteCerts
I0312 09:43:27.778245   21168 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0312 09:43:27.779748   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:27.812392   21168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57368 SSHKeyPath:C:\Users\CSELAB2\.minikube\machines\minikube\id_rsa Username:docker}
I0312 09:43:27.901104   21168 ssh_runner.go:362] scp C:\Users\CSELAB2\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0312 09:43:27.914027   21168 ssh_runner.go:362] scp C:\Users\CSELAB2\.minikube\machines\server.pem --> /etc/docker/server.pem (1180 bytes)
I0312 09:43:27.927922   21168 ssh_runner.go:362] scp C:\Users\CSELAB2\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0312 09:43:27.941830   21168 provision.go:87] duration metric: took 369.6377ms to configureAuth
I0312 09:43:27.941830   21168 ubuntu.go:193] setting minikube options for container-runtime
I0312 09:43:27.942360   21168 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0312 09:43:27.943432   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:27.977223   21168 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0312 09:43:27.977763   21168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa05360] 0xa07ea0 <nil>  [] 0s} 127.0.0.1 57368 <nil> <nil>}
I0312 09:43:27.977763   21168 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0312 09:43:28.107958   21168 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0312 09:43:28.107958   21168 ubuntu.go:71] root file system type: overlay
I0312 09:43:28.107958   21168 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0312 09:43:28.109157   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:28.141514   21168 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0312 09:43:28.141514   21168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa05360] 0xa07ea0 <nil>  [] 0s} 127.0.0.1 57368 <nil> <nil>}
I0312 09:43:28.142070   21168 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0312 09:43:28.278455   21168 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0312 09:43:28.281778   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:28.314059   21168 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0312 09:43:28.314623   21168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa05360] 0xa07ea0 <nil>  [] 0s} 127.0.0.1 57368 <nil> <nil>}
I0312 09:43:28.314623   21168 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0312 09:43:29.109441   21168 main.go:141] libmachine: SSH cmd err, output: <nil>: --- /lib/systemd/system/docker.service	2024-12-17 15:44:19.000000000 +0000
+++ /lib/systemd/system/docker.service.new	2025-03-12 04:13:28.272653791 +0000
@@ -1,46 +1,49 @@
 [Unit]
 Description=Docker Application Container Engine
 Documentation=https://docs.docker.com
-After=network-online.target docker.socket firewalld.service containerd.service time-set.target
-Wants=network-online.target containerd.service
+BindsTo=containerd.service
+After=network-online.target firewalld.service containerd.service
+Wants=network-online.target
 Requires=docker.socket
+StartLimitBurst=3
+StartLimitIntervalSec=60
 
 [Service]
 Type=notify
-# the default is not to use systemd for cgroups because the delegate issues still
-# exists and systemd currently does not support the cgroup feature set required
-# for containers run by docker
-ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock
-ExecReload=/bin/kill -s HUP $MAINPID
-TimeoutStartSec=0
-RestartSec=2
-Restart=always
+Restart=on-failure
 
-# Note that StartLimit* options were moved from "Service" to "Unit" in systemd 229.
-# Both the old, and new location are accepted by systemd 229 and up, so using the old location
-# to make them work for either version of systemd.
-StartLimitBurst=3
 
-# Note that StartLimitInterval was renamed to StartLimitIntervalSec in systemd 230.
-# Both the old, and new name are accepted by systemd 230 and up, so using the old name to make
-# this option work for either version of systemd.
-StartLimitInterval=60s
+
+# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
+# The base configuration already specifies an 'ExecStart=...' command. The first directive
+# here is to clear out that command inherited from the base configuration. Without this,
+# the command from the base configuration and the command specified here are treated as
+# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
+# will catch this invalid input and refuse to start the service with an error like:
+#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.
+
+# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
+# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
+ExecStart=
+ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
+ExecReload=/bin/kill -s HUP $MAINPID
 
 # Having non-zero Limit*s causes performance problems due to accounting overhead
 # in the kernel. We recommend using cgroups to do container-local accounting.
+LimitNOFILE=infinity
 LimitNPROC=infinity
 LimitCORE=infinity
 
-# Comment TasksMax if your systemd version does not support it.
-# Only systemd 226 and above support this option.
+# Uncomment TasksMax if your systemd version supports it.
+# Only systemd 226 and above support this version.
 TasksMax=infinity
+TimeoutStartSec=0
 
 # set delegate yes so that systemd does not reset the cgroups of docker containers
 Delegate=yes
 
 # kill only the docker process, not all processes in the cgroup
 KillMode=process
-OOMScoreAdjust=-500
 
 [Install]
 WantedBy=multi-user.target
Synchronizing state of docker.service with SysV service script with /lib/systemd/systemd-sysv-install.
Executing: /lib/systemd/systemd-sysv-install enable docker

I0312 09:43:29.109441   21168 machine.go:96] duration metric: took 2.0433781s to provisionDockerMachine
I0312 09:43:29.109441   21168 client.go:171] duration metric: took 17.4721188s to LocalClient.Create
I0312 09:43:29.109441   21168 start.go:167] duration metric: took 17.4721188s to libmachine.API.Create "minikube"
I0312 09:43:29.109441   21168 start.go:293] postStartSetup for "minikube" (driver="docker")
I0312 09:43:29.109441   21168 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0312 09:43:29.111663   21168 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0312 09:43:29.112698   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:29.144781   21168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57368 SSHKeyPath:C:\Users\CSELAB2\.minikube\machines\minikube\id_rsa Username:docker}
I0312 09:43:29.237553   21168 ssh_runner.go:195] Run: cat /etc/os-release
I0312 09:43:29.241265   21168 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0312 09:43:29.241265   21168 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0312 09:43:29.241265   21168 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0312 09:43:29.241265   21168 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0312 09:43:29.241265   21168 filesync.go:126] Scanning C:\Users\CSELAB2\.minikube\addons for local assets ...
I0312 09:43:29.241265   21168 filesync.go:126] Scanning C:\Users\CSELAB2\.minikube\files for local assets ...
I0312 09:43:29.241936   21168 start.go:296] duration metric: took 132.4949ms for postStartSetup
I0312 09:43:29.243555   21168 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0312 09:43:29.273186   21168 profile.go:143] Saving config to C:\Users\CSELAB2\.minikube\profiles\minikube\config.json ...
I0312 09:43:29.275832   21168 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0312 09:43:29.276875   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:29.308072   21168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57368 SSHKeyPath:C:\Users\CSELAB2\.minikube\machines\minikube\id_rsa Username:docker}
I0312 09:43:29.391033   21168 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0312 09:43:29.394913   21168 start.go:128] duration metric: took 17.7775947s to createHost
I0312 09:43:29.397013   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
W0312 09:43:29.426522   21168 fix.go:138] unexpected machine state, will restart: <nil>
I0312 09:43:29.426522   21168 machine.go:93] provisionDockerMachine start ...
I0312 09:43:29.428366   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:29.458538   21168 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0312 09:43:29.458538   21168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa05360] 0xa07ea0 <nil>  [] 0s} 127.0.0.1 57368 <nil> <nil>}
I0312 09:43:29.458538   21168 main.go:141] libmachine: About to run SSH command:
hostname
I0312 09:43:29.593472   21168 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0312 09:43:29.593472   21168 ubuntu.go:169] provisioning hostname "minikube"
I0312 09:43:29.594502   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:29.627175   21168 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0312 09:43:29.627175   21168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa05360] 0xa07ea0 <nil>  [] 0s} 127.0.0.1 57368 <nil> <nil>}
I0312 09:43:29.627175   21168 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0312 09:43:29.760748   21168 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0312 09:43:29.761911   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:29.810402   21168 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0312 09:43:29.810402   21168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa05360] 0xa07ea0 <nil>  [] 0s} 127.0.0.1 57368 <nil> <nil>}
I0312 09:43:29.810402   21168 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0312 09:43:29.928662   21168 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0312 09:43:29.928662   21168 ubuntu.go:175] set auth options {CertDir:C:\Users\CSELAB2\.minikube CaCertPath:C:\Users\CSELAB2\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\CSELAB2\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\CSELAB2\.minikube\machines\server.pem ServerKeyPath:C:\Users\CSELAB2\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\CSELAB2\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\CSELAB2\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\CSELAB2\.minikube}
I0312 09:43:29.928662   21168 ubuntu.go:177] setting up certificates
I0312 09:43:29.928662   21168 provision.go:84] configureAuth start
I0312 09:43:29.930454   21168 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0312 09:43:29.965257   21168 provision.go:143] copyHostCerts
I0312 09:43:29.965257   21168 exec_runner.go:144] found C:\Users\CSELAB2\.minikube/ca.pem, removing ...
I0312 09:43:29.965257   21168 exec_runner.go:203] rm: C:\Users\CSELAB2\.minikube\ca.pem
I0312 09:43:29.965773   21168 exec_runner.go:151] cp: C:\Users\CSELAB2\.minikube\certs\ca.pem --> C:\Users\CSELAB2\.minikube/ca.pem (1078 bytes)
I0312 09:43:29.965773   21168 exec_runner.go:144] found C:\Users\CSELAB2\.minikube/cert.pem, removing ...
I0312 09:43:29.965773   21168 exec_runner.go:203] rm: C:\Users\CSELAB2\.minikube\cert.pem
I0312 09:43:29.966293   21168 exec_runner.go:151] cp: C:\Users\CSELAB2\.minikube\certs\cert.pem --> C:\Users\CSELAB2\.minikube/cert.pem (1123 bytes)
I0312 09:43:29.966293   21168 exec_runner.go:144] found C:\Users\CSELAB2\.minikube/key.pem, removing ...
I0312 09:43:29.966293   21168 exec_runner.go:203] rm: C:\Users\CSELAB2\.minikube\key.pem
I0312 09:43:29.966293   21168 exec_runner.go:151] cp: C:\Users\CSELAB2\.minikube\certs\key.pem --> C:\Users\CSELAB2\.minikube/key.pem (1675 bytes)
I0312 09:43:29.966813   21168 provision.go:117] generating server cert: C:\Users\CSELAB2\.minikube\machines\server.pem ca-key=C:\Users\CSELAB2\.minikube\certs\ca.pem private-key=C:\Users\CSELAB2\.minikube\certs\ca-key.pem org=CSELAB2.minikube san=[127.0.0.1 172.17.0.2 localhost minikube]
I0312 09:43:30.057810   21168 provision.go:177] copyRemoteCerts
I0312 09:43:30.059815   21168 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0312 09:43:30.060816   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:30.090971   21168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57368 SSHKeyPath:C:\Users\CSELAB2\.minikube\machines\minikube\id_rsa Username:docker}
I0312 09:43:30.183067   21168 ssh_runner.go:362] scp C:\Users\CSELAB2\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1078 bytes)
I0312 09:43:30.196283   21168 ssh_runner.go:362] scp C:\Users\CSELAB2\.minikube\machines\server.pem --> /etc/docker/server.pem (1184 bytes)
I0312 09:43:30.209604   21168 ssh_runner.go:362] scp C:\Users\CSELAB2\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I0312 09:43:30.222573   21168 provision.go:87] duration metric: took 293.9102ms to configureAuth
I0312 09:43:30.222573   21168 ubuntu.go:193] setting minikube options for container-runtime
I0312 09:43:30.222573   21168 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0312 09:43:30.223884   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:30.259650   21168 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0312 09:43:30.260186   21168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa05360] 0xa07ea0 <nil>  [] 0s} 127.0.0.1 57368 <nil> <nil>}
I0312 09:43:30.260186   21168 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0312 09:43:30.383262   21168 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0312 09:43:30.383262   21168 ubuntu.go:71] root file system type: overlay
I0312 09:43:30.383262   21168 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0312 09:43:30.384332   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:30.415206   21168 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0312 09:43:30.415206   21168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa05360] 0xa07ea0 <nil>  [] 0s} 127.0.0.1 57368 <nil> <nil>}
I0312 09:43:30.415206   21168 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0312 09:43:30.555963   21168 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0312 09:43:30.557173   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:30.588156   21168 main.go:141] libmachine: SSH binary not found, using native Go implementation
I0312 09:43:30.588683   21168 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0xa05360] 0xa07ea0 <nil>  [] 0s} 127.0.0.1 57368 <nil> <nil>}
I0312 09:43:30.588683   21168 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0312 09:43:30.712126   21168 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0312 09:43:30.712126   21168 machine.go:96] duration metric: took 1.2856044s to provisionDockerMachine
I0312 09:43:30.712126   21168 start.go:293] postStartSetup for "minikube" (driver="docker")
I0312 09:43:30.712126   21168 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0312 09:43:30.714289   21168 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0312 09:43:30.715351   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:30.744746   21168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57368 SSHKeyPath:C:\Users\CSELAB2\.minikube\machines\minikube\id_rsa Username:docker}
I0312 09:43:30.846726   21168 ssh_runner.go:195] Run: cat /etc/os-release
I0312 09:43:30.849657   21168 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0312 09:43:30.849657   21168 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0312 09:43:30.849657   21168 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0312 09:43:30.849657   21168 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0312 09:43:30.849657   21168 filesync.go:126] Scanning C:\Users\CSELAB2\.minikube\addons for local assets ...
I0312 09:43:30.849657   21168 filesync.go:126] Scanning C:\Users\CSELAB2\.minikube\files for local assets ...
I0312 09:43:30.849657   21168 start.go:296] duration metric: took 137.5311ms for postStartSetup
I0312 09:43:30.851815   21168 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0312 09:43:30.852385   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:30.884385   21168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57368 SSHKeyPath:C:\Users\CSELAB2\.minikube\machines\minikube\id_rsa Username:docker}
I0312 09:43:30.969404   21168 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0312 09:43:30.972535   21168 fix.go:56] duration metric: took 36.6162232s for fixHost
I0312 09:43:30.972535   21168 start.go:83] releasing machines lock for "minikube", held for 36.6167644s
I0312 09:43:30.979348   21168 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0312 09:43:31.011248   21168 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0312 09:43:31.012336   21168 ssh_runner.go:195] Run: cat /version.json
I0312 09:43:31.013403   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:31.016601   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:31.043448   21168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57368 SSHKeyPath:C:\Users\CSELAB2\.minikube\machines\minikube\id_rsa Username:docker}
I0312 09:43:31.045033   21168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57368 SSHKeyPath:C:\Users\CSELAB2\.minikube\machines\minikube\id_rsa Username:docker}
W0312 09:43:31.127990   21168 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0312 09:43:31.130699   21168 ssh_runner.go:195] Run: systemctl --version
I0312 09:43:31.138086   21168 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0312 09:43:31.144548   21168 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0312 09:43:31.150906   21168 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0312 09:43:31.152488   21168 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0312 09:43:31.171583   21168 cni.go:262] disabled [/etc/cni/net.d/100-crio-bridge.conf, /etc/cni/net.d/87-podman-bridge.conflist] bridge cni config(s)
I0312 09:43:31.171583   21168 start.go:495] detecting cgroup driver to use...
I0312 09:43:31.171583   21168 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0312 09:43:31.171583   21168 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0312 09:43:31.183188   21168 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0312 09:43:31.191741   21168 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0312 09:43:31.197960   21168 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0312 09:43:31.200087   21168 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0312 09:43:31.208157   21168 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0312 09:43:31.216435   21168 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0312 09:43:31.223889   21168 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0312 09:43:31.231943   21168 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0312 09:43:31.240479   21168 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0312 09:43:31.248480   21168 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0312 09:43:31.257543   21168 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0312 09:43:31.265948   21168 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0312 09:43:31.273890   21168 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0312 09:43:31.283343   21168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0312 09:43:31.351632   21168 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0312 09:43:31.482696   21168 start.go:495] detecting cgroup driver to use...
I0312 09:43:31.482696   21168 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0312 09:43:31.486026   21168 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0312 09:43:31.493500   21168 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0312 09:43:31.495656   21168 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0312 09:43:31.503819   21168 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0312 09:43:31.515227   21168 ssh_runner.go:195] Run: which cri-dockerd
I0312 09:43:31.519890   21168 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0312 09:43:31.525279   21168 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0312 09:43:31.537400   21168 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0312 09:43:31.610412   21168 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0312 09:43:31.679178   21168 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0312 09:43:31.679684   21168 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0312 09:43:31.692253   21168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
W0312 09:43:31.692253   21168 out.go:270] ❗  Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0312 09:43:31.692253   21168 out.go:270] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0312 09:43:31.772412   21168 ssh_runner.go:195] Run: sudo systemctl restart docker
I0312 09:43:32.104981   21168 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0312 09:43:32.114374   21168 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0312 09:43:32.122636   21168 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0312 09:43:32.192399   21168 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0312 09:43:32.273978   21168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0312 09:43:32.353509   21168 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0312 09:43:32.363266   21168 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0312 09:43:32.371796   21168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0312 09:43:32.456560   21168 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0312 09:43:32.499140   21168 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0312 09:43:32.501286   21168 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0312 09:43:32.505209   21168 start.go:563] Will wait 60s for crictl version
I0312 09:43:32.506892   21168 ssh_runner.go:195] Run: which crictl
I0312 09:43:32.512050   21168 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0312 09:43:32.536320   21168 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0312 09:43:32.537388   21168 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0312 09:43:32.556040   21168 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0312 09:43:32.573066   21168 out.go:235] 🐳  Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0312 09:43:32.574629   21168 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0312 09:43:32.643933   21168 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0312 09:43:32.646042   21168 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0312 09:43:32.649221   21168 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0312 09:43:32.657440   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0312 09:43:32.687632   21168 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.17.0.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\CSELAB2:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0312 09:43:32.687632   21168 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0312 09:43:32.688767   21168 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0312 09:43:32.703728   21168 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0312 09:43:32.703728   21168 docker.go:619] Images already preloaded, skipping extraction
I0312 09:43:32.705354   21168 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0312 09:43:32.721223   21168 docker.go:689] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0312 09:43:32.721223   21168 cache_images.go:84] Images are preloaded, skipping loading
I0312 09:43:32.721223   21168 kubeadm.go:934] updating node { 172.17.0.2 8443 v1.32.0 docker true true} ...
I0312 09:43:32.721747   21168 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=172.17.0.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0312 09:43:32.723310   21168 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0312 09:43:32.752891   21168 cni.go:84] Creating CNI manager for ""
I0312 09:43:32.752891   21168 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0312 09:43:32.752891   21168 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0312 09:43:32.752891   21168 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:172.17.0.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "172.17.0.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:172.17.0.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0312 09:43:32.753508   21168 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 172.17.0.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "172.17.0.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "172.17.0.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0312 09:43:32.755636   21168 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0312 09:43:32.761449   21168 binaries.go:44] Found k8s binaries, skipping transfer
I0312 09:43:32.763086   21168 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0312 09:43:32.770322   21168 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (305 bytes)
I0312 09:43:32.780635   21168 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0312 09:43:32.790161   21168 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2280 bytes)
I0312 09:43:32.802247   21168 ssh_runner.go:195] Run: grep 172.17.0.2	control-plane.minikube.internal$ /etc/hosts
I0312 09:43:32.805892   21168 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "172.17.0.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0312 09:43:32.814241   21168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0312 09:43:32.893018   21168 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0312 09:43:32.904238   21168 certs.go:68] Setting up C:\Users\CSELAB2\.minikube\profiles\minikube for IP: 172.17.0.2
I0312 09:43:32.904238   21168 certs.go:194] generating shared ca certs ...
I0312 09:43:32.904238   21168 certs.go:226] acquiring lock for ca certs: {Name:mkb84c4dc3f9b35223acadf1670bd7b51509cf87 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0312 09:43:32.904238   21168 certs.go:240] generating "minikubeCA" ca cert: C:\Users\CSELAB2\.minikube\ca.key
I0312 09:43:32.949471   21168 crypto.go:156] Writing cert to C:\Users\CSELAB2\.minikube\ca.crt ...
I0312 09:43:32.949471   21168 lock.go:35] WriteFile acquiring C:\Users\CSELAB2\.minikube\ca.crt: {Name:mk065ee174a6f28407ef86ff3b093817caecd7a7 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0312 09:43:32.950470   21168 crypto.go:164] Writing key to C:\Users\CSELAB2\.minikube\ca.key ...
I0312 09:43:32.950470   21168 lock.go:35] WriteFile acquiring C:\Users\CSELAB2\.minikube\ca.key: {Name:mk5658c81cf17430146ff6c2cbcc9fe4bce5d47d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0312 09:43:32.950470   21168 certs.go:240] generating "proxyClientCA" ca cert: C:\Users\CSELAB2\.minikube\proxy-client-ca.key
I0312 09:43:32.972494   21168 crypto.go:156] Writing cert to C:\Users\CSELAB2\.minikube\proxy-client-ca.crt ...
I0312 09:43:32.972494   21168 lock.go:35] WriteFile acquiring C:\Users\CSELAB2\.minikube\proxy-client-ca.crt: {Name:mk9f720c811f3d52f7ce0e95f23c9dfa09dc9689 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0312 09:43:32.973495   21168 crypto.go:164] Writing key to C:\Users\CSELAB2\.minikube\proxy-client-ca.key ...
I0312 09:43:32.973495   21168 lock.go:35] WriteFile acquiring C:\Users\CSELAB2\.minikube\proxy-client-ca.key: {Name:mkb249c54bfa8ad73ac176fb1a8eacf2937bd810 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0312 09:43:32.973495   21168 certs.go:256] generating profile certs ...
I0312 09:43:32.973495   21168 certs.go:363] generating signed profile cert for "minikube-user": C:\Users\CSELAB2\.minikube\profiles\minikube\client.key
I0312 09:43:32.973495   21168 crypto.go:68] Generating cert C:\Users\CSELAB2\.minikube\profiles\minikube\client.crt with IP's: []
I0312 09:43:33.022115   21168 crypto.go:156] Writing cert to C:\Users\CSELAB2\.minikube\profiles\minikube\client.crt ...
I0312 09:43:33.022115   21168 lock.go:35] WriteFile acquiring C:\Users\CSELAB2\.minikube\profiles\minikube\client.crt: {Name:mk2ae154d155ee0227fd5579e5f98134a6fe8fb5 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0312 09:43:33.023098   21168 crypto.go:164] Writing key to C:\Users\CSELAB2\.minikube\profiles\minikube\client.key ...
I0312 09:43:33.023098   21168 lock.go:35] WriteFile acquiring C:\Users\CSELAB2\.minikube\profiles\minikube\client.key: {Name:mkb7d3310fdbecf9e8ad56e8cd2313f557cd37a6 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0312 09:43:33.024099   21168 certs.go:363] generating signed profile cert for "minikube": C:\Users\CSELAB2\.minikube\profiles\minikube\apiserver.key.f55e2435
I0312 09:43:33.024099   21168 crypto.go:68] Generating cert C:\Users\CSELAB2\.minikube\profiles\minikube\apiserver.crt.f55e2435 with IP's: [10.96.0.1 127.0.0.1 10.0.0.1 172.17.0.2]
I0312 09:43:33.046116   21168 crypto.go:156] Writing cert to C:\Users\CSELAB2\.minikube\profiles\minikube\apiserver.crt.f55e2435 ...
I0312 09:43:33.046116   21168 lock.go:35] WriteFile acquiring C:\Users\CSELAB2\.minikube\profiles\minikube\apiserver.crt.f55e2435: {Name:mkc31e5a2c0f78fcbff133860d567526e1bce96a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0312 09:43:33.046116   21168 crypto.go:164] Writing key to C:\Users\CSELAB2\.minikube\profiles\minikube\apiserver.key.f55e2435 ...
I0312 09:43:33.046116   21168 lock.go:35] WriteFile acquiring C:\Users\CSELAB2\.minikube\profiles\minikube\apiserver.key.f55e2435: {Name:mk96bd4a12168620ce5e7c151c6334ae8a248c7c Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0312 09:43:33.047112   21168 certs.go:381] copying C:\Users\CSELAB2\.minikube\profiles\minikube\apiserver.crt.f55e2435 -> C:\Users\CSELAB2\.minikube\profiles\minikube\apiserver.crt
I0312 09:43:33.059749   21168 certs.go:385] copying C:\Users\CSELAB2\.minikube\profiles\minikube\apiserver.key.f55e2435 -> C:\Users\CSELAB2\.minikube\profiles\minikube\apiserver.key
I0312 09:43:33.060537   21168 certs.go:363] generating signed profile cert for "aggregator": C:\Users\CSELAB2\.minikube\profiles\minikube\proxy-client.key
I0312 09:43:33.060537   21168 crypto.go:68] Generating cert C:\Users\CSELAB2\.minikube\profiles\minikube\proxy-client.crt with IP's: []
I0312 09:43:33.189334   21168 crypto.go:156] Writing cert to C:\Users\CSELAB2\.minikube\profiles\minikube\proxy-client.crt ...
I0312 09:43:33.189334   21168 lock.go:35] WriteFile acquiring C:\Users\CSELAB2\.minikube\profiles\minikube\proxy-client.crt: {Name:mk98b174b9f603172a054da1f4cb41fac01e564d Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0312 09:43:33.190326   21168 crypto.go:164] Writing key to C:\Users\CSELAB2\.minikube\profiles\minikube\proxy-client.key ...
I0312 09:43:33.190326   21168 lock.go:35] WriteFile acquiring C:\Users\CSELAB2\.minikube\profiles\minikube\proxy-client.key: {Name:mk80cf68d81da70a499e958c2415cba60146ad0e Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0312 09:43:33.198863   21168 certs.go:484] found cert: C:\Users\CSELAB2\.minikube\certs\ca-key.pem (1679 bytes)
I0312 09:43:33.198863   21168 certs.go:484] found cert: C:\Users\CSELAB2\.minikube\certs\ca.pem (1078 bytes)
I0312 09:43:33.198863   21168 certs.go:484] found cert: C:\Users\CSELAB2\.minikube\certs\cert.pem (1123 bytes)
I0312 09:43:33.199855   21168 certs.go:484] found cert: C:\Users\CSELAB2\.minikube\certs\key.pem (1675 bytes)
I0312 09:43:33.199855   21168 ssh_runner.go:362] scp C:\Users\CSELAB2\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0312 09:43:33.219256   21168 ssh_runner.go:362] scp C:\Users\CSELAB2\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0312 09:43:33.233255   21168 ssh_runner.go:362] scp C:\Users\CSELAB2\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0312 09:43:33.248439   21168 ssh_runner.go:362] scp C:\Users\CSELAB2\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0312 09:43:33.261234   21168 ssh_runner.go:362] scp C:\Users\CSELAB2\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0312 09:43:33.274209   21168 ssh_runner.go:362] scp C:\Users\CSELAB2\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0312 09:43:33.287632   21168 ssh_runner.go:362] scp C:\Users\CSELAB2\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0312 09:43:33.302594   21168 ssh_runner.go:362] scp C:\Users\CSELAB2\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0312 09:43:33.316717   21168 ssh_runner.go:362] scp C:\Users\CSELAB2\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0312 09:43:33.330616   21168 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0312 09:43:33.345728   21168 ssh_runner.go:195] Run: openssl version
I0312 09:43:33.351828   21168 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0312 09:43:33.360099   21168 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0312 09:43:33.362811   21168 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Mar 12 04:13 /usr/share/ca-certificates/minikubeCA.pem
I0312 09:43:33.364969   21168 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0312 09:43:33.371017   21168 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0312 09:43:33.378983   21168 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0312 09:43:33.381799   21168 certs.go:399] 'apiserver-kubelet-client' cert doesn't exist, likely first start: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt: Process exited with status 1
stdout:

stderr:
stat: cannot statx '/var/lib/minikube/certs/apiserver-kubelet-client.crt': No such file or directory
I0312 09:43:33.381799   21168 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:172.17.0.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\CSELAB2:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0312 09:43:33.382912   21168 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0312 09:43:33.397596   21168 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0312 09:43:33.407792   21168 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I0312 09:43:33.415506   21168 kubeadm.go:214] ignoring SystemVerification for kubeadm because of docker driver
I0312 09:43:33.417098   21168 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I0312 09:43:33.423861   21168 kubeadm.go:155] config check failed, skipping stale config cleanup: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
ls: cannot access '/etc/kubernetes/admin.conf': No such file or directory
ls: cannot access '/etc/kubernetes/kubelet.conf': No such file or directory
ls: cannot access '/etc/kubernetes/controller-manager.conf': No such file or directory
ls: cannot access '/etc/kubernetes/scheduler.conf': No such file or directory
I0312 09:43:33.423861   21168 kubeadm.go:157] found existing configuration files:

I0312 09:43:33.425947   21168 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I0312 09:43:33.431568   21168 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/admin.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/admin.conf: No such file or directory
I0312 09:43:33.433839   21168 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/admin.conf
I0312 09:43:33.443203   21168 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I0312 09:43:33.448509   21168 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/kubelet.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/kubelet.conf: No such file or directory
I0312 09:43:33.450707   21168 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/kubelet.conf
I0312 09:43:33.458339   21168 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I0312 09:43:33.464081   21168 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/controller-manager.conf: No such file or directory
I0312 09:43:33.466312   21168 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I0312 09:43:33.474249   21168 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I0312 09:43:33.479515   21168 kubeadm.go:163] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 2
stdout:

stderr:
grep: /etc/kubernetes/scheduler.conf: No such file or directory
I0312 09:43:33.481658   21168 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I0312 09:43:33.486629   21168 ssh_runner.go:286] Start: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.32.0:$PATH" kubeadm init --config /var/tmp/minikube/kubeadm.yaml  --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests,DirAvailable--var-lib-minikube,DirAvailable--var-lib-minikube-etcd,FileAvailable--etc-kubernetes-manifests-kube-scheduler.yaml,FileAvailable--etc-kubernetes-manifests-kube-apiserver.yaml,FileAvailable--etc-kubernetes-manifests-kube-controller-manager.yaml,FileAvailable--etc-kubernetes-manifests-etcd.yaml,Port-10250,Swap,NumCPU,Mem,SystemVerification,FileContent--proc-sys-net-bridge-bridge-nf-call-iptables"
I0312 09:43:33.523901   21168 kubeadm.go:310] 	[WARNING Swap]: swap is supported for cgroup v2 only. The kubelet must be properly configured to use swap. Please refer to https://kubernetes.io/docs/concepts/architecture/nodes/#swap-memory, or disable swap on the node
I0312 09:43:33.526704   21168 kubeadm.go:310] 	[WARNING SystemVerification]: cgroups v1 support is in maintenance mode, please migrate to cgroups v2
I0312 09:43:33.566045   21168 kubeadm.go:310] 	[WARNING Service-Kubelet]: kubelet service is not enabled, please run 'systemctl enable kubelet.service'
I0312 09:43:41.068607   21168 kubeadm.go:310] [init] Using Kubernetes version: v1.32.0
I0312 09:43:41.068607   21168 kubeadm.go:310] [preflight] Running pre-flight checks
I0312 09:43:41.068607   21168 kubeadm.go:310] [preflight] Pulling images required for setting up a Kubernetes cluster
I0312 09:43:41.068607   21168 kubeadm.go:310] [preflight] This might take a minute or two, depending on the speed of your internet connection
I0312 09:43:41.068607   21168 kubeadm.go:310] [preflight] You can also perform this action beforehand using 'kubeadm config images pull'
I0312 09:43:41.068607   21168 kubeadm.go:310] [certs] Using certificateDir folder "/var/lib/minikube/certs"
I0312 09:43:41.069689   21168 out.go:235]     ▪ Generating certificates and keys ...
I0312 09:43:41.070205   21168 kubeadm.go:310] [certs] Using existing ca certificate authority
I0312 09:43:41.070205   21168 kubeadm.go:310] [certs] Using existing apiserver certificate and key on disk
I0312 09:43:41.070205   21168 kubeadm.go:310] [certs] Generating "apiserver-kubelet-client" certificate and key
I0312 09:43:41.070205   21168 kubeadm.go:310] [certs] Generating "front-proxy-ca" certificate and key
I0312 09:43:41.070205   21168 kubeadm.go:310] [certs] Generating "front-proxy-client" certificate and key
I0312 09:43:41.070205   21168 kubeadm.go:310] [certs] Generating "etcd/ca" certificate and key
I0312 09:43:41.070205   21168 kubeadm.go:310] [certs] Generating "etcd/server" certificate and key
I0312 09:43:41.070742   21168 kubeadm.go:310] [certs] etcd/server serving cert is signed for DNS names [localhost minikube] and IPs [172.17.0.2 127.0.0.1 ::1]
I0312 09:43:41.070742   21168 kubeadm.go:310] [certs] Generating "etcd/peer" certificate and key
I0312 09:43:41.070742   21168 kubeadm.go:310] [certs] etcd/peer serving cert is signed for DNS names [localhost minikube] and IPs [172.17.0.2 127.0.0.1 ::1]
I0312 09:43:41.070742   21168 kubeadm.go:310] [certs] Generating "etcd/healthcheck-client" certificate and key
I0312 09:43:41.070742   21168 kubeadm.go:310] [certs] Generating "apiserver-etcd-client" certificate and key
I0312 09:43:41.070742   21168 kubeadm.go:310] [certs] Generating "sa" key and public key
I0312 09:43:41.070742   21168 kubeadm.go:310] [kubeconfig] Using kubeconfig folder "/etc/kubernetes"
I0312 09:43:41.070742   21168 kubeadm.go:310] [kubeconfig] Writing "admin.conf" kubeconfig file
I0312 09:43:41.071257   21168 kubeadm.go:310] [kubeconfig] Writing "super-admin.conf" kubeconfig file
I0312 09:43:41.071257   21168 kubeadm.go:310] [kubeconfig] Writing "kubelet.conf" kubeconfig file
I0312 09:43:41.071257   21168 kubeadm.go:310] [kubeconfig] Writing "controller-manager.conf" kubeconfig file
I0312 09:43:41.071257   21168 kubeadm.go:310] [kubeconfig] Writing "scheduler.conf" kubeconfig file
I0312 09:43:41.071257   21168 kubeadm.go:310] [etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
I0312 09:43:41.071257   21168 kubeadm.go:310] [control-plane] Using manifest folder "/etc/kubernetes/manifests"
I0312 09:43:41.072284   21168 out.go:235]     ▪ Booting up control plane ...
I0312 09:43:41.072828   21168 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-apiserver"
I0312 09:43:41.072828   21168 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-controller-manager"
I0312 09:43:41.072828   21168 kubeadm.go:310] [control-plane] Creating static Pod manifest for "kube-scheduler"
I0312 09:43:41.072828   21168 kubeadm.go:310] [kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
I0312 09:43:41.072828   21168 kubeadm.go:310] [kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
I0312 09:43:41.072828   21168 kubeadm.go:310] [kubelet-start] Starting the kubelet
I0312 09:43:41.072828   21168 kubeadm.go:310] [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests"
I0312 09:43:41.073341   21168 kubeadm.go:310] [kubelet-check] Waiting for a healthy kubelet at http://127.0.0.1:10248/healthz. This can take up to 4m0s
I0312 09:43:41.073341   21168 kubeadm.go:310] [kubelet-check] The kubelet is healthy after 502.055302ms
I0312 09:43:41.073341   21168 kubeadm.go:310] [api-check] Waiting for a healthy API server. This can take up to 4m0s
I0312 09:43:41.073341   21168 kubeadm.go:310] [api-check] The API server is healthy after 4.00073576s
I0312 09:43:41.073341   21168 kubeadm.go:310] [upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
I0312 09:43:41.073854   21168 kubeadm.go:310] [kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
I0312 09:43:41.073854   21168 kubeadm.go:310] [upload-certs] Skipping phase. Please see --upload-certs
I0312 09:43:41.073854   21168 kubeadm.go:310] [mark-control-plane] Marking the node minikube as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
I0312 09:43:41.073854   21168 kubeadm.go:310] [bootstrap-token] Using token: i0trhj.zawk30ujhbmhxb5y
I0312 09:43:41.074887   21168 out.go:235]     ▪ Configuring RBAC rules ...
I0312 09:43:41.075401   21168 kubeadm.go:310] [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
I0312 09:43:41.075401   21168 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
I0312 09:43:41.075401   21168 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
I0312 09:43:41.075401   21168 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
I0312 09:43:41.075915   21168 kubeadm.go:310] [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
I0312 09:43:41.075915   21168 kubeadm.go:310] [bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
I0312 09:43:41.075915   21168 kubeadm.go:310] [kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
I0312 09:43:41.075915   21168 kubeadm.go:310] [addons] Applied essential addon: CoreDNS
I0312 09:43:41.075915   21168 kubeadm.go:310] [addons] Applied essential addon: kube-proxy
I0312 09:43:41.075915   21168 kubeadm.go:310] 
I0312 09:43:41.075915   21168 kubeadm.go:310] Your Kubernetes control-plane has initialized successfully!
I0312 09:43:41.075915   21168 kubeadm.go:310] 
I0312 09:43:41.076427   21168 kubeadm.go:310] To start using your cluster, you need to run the following as a regular user:
I0312 09:43:41.076427   21168 kubeadm.go:310] 
I0312 09:43:41.076427   21168 kubeadm.go:310]   mkdir -p $HOME/.kube
I0312 09:43:41.076427   21168 kubeadm.go:310]   sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
I0312 09:43:41.076427   21168 kubeadm.go:310]   sudo chown $(id -u):$(id -g) $HOME/.kube/config
I0312 09:43:41.076427   21168 kubeadm.go:310] 
I0312 09:43:41.076427   21168 kubeadm.go:310] Alternatively, if you are the root user, you can run:
I0312 09:43:41.076427   21168 kubeadm.go:310] 
I0312 09:43:41.076427   21168 kubeadm.go:310]   export KUBECONFIG=/etc/kubernetes/admin.conf
I0312 09:43:41.076427   21168 kubeadm.go:310] 
I0312 09:43:41.076427   21168 kubeadm.go:310] You should now deploy a pod network to the cluster.
I0312 09:43:41.076427   21168 kubeadm.go:310] Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
I0312 09:43:41.076427   21168 kubeadm.go:310]   https://kubernetes.io/docs/concepts/cluster-administration/addons/
I0312 09:43:41.076427   21168 kubeadm.go:310] 
I0312 09:43:41.076939   21168 kubeadm.go:310] You can now join any number of control-plane nodes by copying certificate authorities
I0312 09:43:41.076939   21168 kubeadm.go:310] and service account keys on each node and then running the following as root:
I0312 09:43:41.076939   21168 kubeadm.go:310] 
I0312 09:43:41.076939   21168 kubeadm.go:310]   kubeadm join control-plane.minikube.internal:8443 --token i0trhj.zawk30ujhbmhxb5y \
I0312 09:43:41.076939   21168 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:b5d906095942b00aa73c6ada8426fb73d111c4ab67dbcac4df11f254670861bd \
I0312 09:43:41.076939   21168 kubeadm.go:310] 	--control-plane 
I0312 09:43:41.076939   21168 kubeadm.go:310] 
I0312 09:43:41.076939   21168 kubeadm.go:310] Then you can join any number of worker nodes by running the following on each as root:
I0312 09:43:41.076939   21168 kubeadm.go:310] 
I0312 09:43:41.076939   21168 kubeadm.go:310] kubeadm join control-plane.minikube.internal:8443 --token i0trhj.zawk30ujhbmhxb5y \
I0312 09:43:41.077449   21168 kubeadm.go:310] 	--discovery-token-ca-cert-hash sha256:b5d906095942b00aa73c6ada8426fb73d111c4ab67dbcac4df11f254670861bd 
I0312 09:43:41.077449   21168 cni.go:84] Creating CNI manager for ""
I0312 09:43:41.077449   21168 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0312 09:43:41.078503   21168 out.go:177] 🔗  Configuring bridge CNI (Container Networking Interface) ...
I0312 09:43:41.081137   21168 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I0312 09:43:41.088751   21168 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (496 bytes)
I0312 09:43:41.099937   21168 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I0312 09:43:41.102551   21168 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl --kubeconfig=/var/lib/minikube/kubeconfig label --overwrite nodes minikube minikube.k8s.io/updated_at=2025_03_12T09_43_41_0700 minikube.k8s.io/version=v1.35.0 minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty minikube.k8s.io/name=minikube minikube.k8s.io/primary=true
I0312 09:43:41.103067   21168 ssh_runner.go:195] Run: sudo /var/lib/minikube/binaries/v1.32.0/kubectl create clusterrolebinding minikube-rbac --clusterrole=cluster-admin --serviceaccount=kube-system:default --kubeconfig=/var/lib/minikube/kubeconfig
I0312 09:43:41.107150   21168 ops.go:34] apiserver oom_adj: -16
I0312 09:43:41.174101   21168 kubeadm.go:1113] duration metric: took 73.6422ms to wait for elevateKubeSystemPrivileges
I0312 09:43:41.174101   21168 kubeadm.go:394] duration metric: took 7.7923017s to StartCluster
I0312 09:43:41.174101   21168 settings.go:142] acquiring lock: {Name:mk2ae4aec6943fad4bc722479fb7b9e25051cc24 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0312 09:43:41.174101   21168 settings.go:150] Updating kubeconfig:  C:\Users\CSELAB2\.kube\config
I0312 09:43:41.187830   21168 lock.go:35] WriteFile acquiring C:\Users\CSELAB2\.kube\config: {Name:mkb0cfc42700ef51689b03a3890a4f5eb20634e4 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0312 09:43:41.188366   21168 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0312 09:43:41.188366   21168 start.go:235] Will wait 6m0s for node &{Name: IP:172.17.0.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0312 09:43:41.188366   21168 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0312 09:43:41.188366   21168 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0312 09:43:41.188366   21168 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0312 09:43:41.188893   21168 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0312 09:43:41.188893   21168 host.go:66] Checking if "minikube" exists ...
I0312 09:43:41.188893   21168 out.go:177] 🔎  Verifying Kubernetes components...
I0312 09:43:41.188893   21168 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0312 09:43:41.192109   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0312 09:43:41.192109   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0312 09:43:41.192704   21168 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0312 09:43:41.233916   21168 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0312 09:43:41.234956   21168 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0312 09:43:41.234956   21168 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0312 09:43:41.235991   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:41.242362   21168 addons.go:238] Setting addon default-storageclass=true in "minikube"
I0312 09:43:41.242362   21168 host.go:66] Checking if "minikube" exists ...
I0312 09:43:41.245479   21168 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0312 09:43:41.274663   21168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57368 SSHKeyPath:C:\Users\CSELAB2\.minikube\machines\minikube\id_rsa Username:docker}
I0312 09:43:41.280948   21168 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0312 09:43:41.280948   21168 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0312 09:43:41.282504   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0312 09:43:41.302715   21168 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0312 09:43:41.312216   21168 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0312 09:43:41.314357   21168 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:57368 SSHKeyPath:C:\Users\CSELAB2\.minikube\machines\minikube\id_rsa Username:docker}
I0312 09:43:41.344871   21168 api_server.go:52] waiting for apiserver process to appear ...
I0312 09:43:41.347097   21168 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0312 09:43:41.354668   21168 api_server.go:72] duration metric: took 166.302ms to wait for apiserver process to appear ...
I0312 09:43:41.354668   21168 api_server.go:88] waiting for apiserver healthz status ...
I0312 09:43:41.354668   21168 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:57372/healthz ...
I0312 09:43:41.360106   21168 api_server.go:279] https://127.0.0.1:57372/healthz returned 200:
ok
I0312 09:43:41.364396   21168 api_server.go:141] control plane version: v1.32.0
I0312 09:43:41.364396   21168 api_server.go:131] duration metric: took 9.7284ms to wait for apiserver health ...
I0312 09:43:41.364396   21168 system_pods.go:43] waiting for kube-system pods to appear ...
I0312 09:43:41.367634   21168 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0312 09:43:41.377752   21168 system_pods.go:59] 4 kube-system pods found
I0312 09:43:41.378274   21168 system_pods.go:61] "etcd-minikube" [82fd10db-877f-45e2-a774-1dba2463ab57] Pending
I0312 09:43:41.378274   21168 system_pods.go:61] "kube-apiserver-minikube" [8bc61ad9-ab35-4d63-85ed-7d91dc4e3b31] Pending
I0312 09:43:41.378274   21168 system_pods.go:61] "kube-controller-manager-minikube" [d1a3b1ba-53ae-4bca-a6d8-c3fa53511838] Pending
I0312 09:43:41.378274   21168 system_pods.go:61] "kube-scheduler-minikube" [6f0e3018-954f-471f-8be0-45356b3a0fad] Pending
I0312 09:43:41.378274   21168 system_pods.go:74] duration metric: took 13.8781ms to wait for pod list to return data ...
I0312 09:43:41.378274   21168 kubeadm.go:582] duration metric: took 189.9085ms to wait for: map[apiserver:true system_pods:true]
I0312 09:43:41.378274   21168 node_conditions.go:102] verifying NodePressure condition ...
I0312 09:43:41.381968   21168 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0312 09:43:41.381968   21168 node_conditions.go:123] node cpu capacity is 20
I0312 09:43:41.381968   21168 node_conditions.go:105] duration metric: took 3.6936ms to run NodePressure ...
I0312 09:43:41.381968   21168 start.go:241] waiting for startup goroutines ...
I0312 09:43:41.419104   21168 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0312 09:43:41.696041   21168 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0312 09:43:41.696572   21168 addons.go:514] duration metric: took 508.2061ms for enable addons: enabled=[storage-provisioner default-storageclass]
I0312 09:43:41.696572   21168 start.go:246] waiting for cluster config update ...
I0312 09:43:41.696572   21168 start.go:255] writing updated cluster config ...
I0312 09:43:41.698703   21168 ssh_runner.go:195] Run: rm -f paused
I0312 09:43:41.956909   21168 start.go:600] kubectl: 1.31.4, cluster: 1.32.0 (minor skew: 1)
I0312 09:43:41.957671   21168 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Mar 12 04:13:31 minikube systemd[1]: Starting Docker Application Container Engine...
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.503323653Z" level=info msg="Starting up"
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.504227633Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.538922869Z" level=info msg="[graphdriver] using prior storage driver: overlay2"
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.540771095Z" level=info msg="Loading containers: start."
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.605427624Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.18.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.638719101Z" level=info msg="Loading containers: done."
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.655861089Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.655892133Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.655896731Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.655899464Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.655967474Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.656002111Z" level=info msg="Daemon has completed initialization"
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.689115110Z" level=info msg="API listen on /var/run/docker.sock"
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.689149735Z" level=info msg="API listen on [::]:2376"
Mar 12 04:13:31 minikube systemd[1]: Started Docker Application Container Engine.
Mar 12 04:13:31 minikube systemd[1]: Stopping Docker Application Container Engine...
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.778266963Z" level=info msg="Processing signal 'terminated'"
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.779366025Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.780626188Z" level=info msg="Daemon shutdown complete"
Mar 12 04:13:31 minikube dockerd[1213]: time="2025-03-12T04:13:31.780680821Z" level=info msg="stopping event stream following graceful shutdown" error="context canceled" module=libcontainerd namespace=plugins.moby
Mar 12 04:13:31 minikube systemd[1]: docker.service: Deactivated successfully.
Mar 12 04:13:31 minikube systemd[1]: Stopped Docker Application Container Engine.
Mar 12 04:13:31 minikube systemd[1]: Starting Docker Application Container Engine...
Mar 12 04:13:31 minikube dockerd[1492]: time="2025-03-12T04:13:31.805523916Z" level=info msg="Starting up"
Mar 12 04:13:31 minikube dockerd[1492]: time="2025-03-12T04:13:31.806238734Z" level=info msg="OTEL tracing is not configured, using no-op tracer provider"
Mar 12 04:13:31 minikube dockerd[1492]: time="2025-03-12T04:13:31.822171509Z" level=info msg="[graphdriver] trying configured driver: overlay2"
Mar 12 04:13:31 minikube dockerd[1492]: time="2025-03-12T04:13:31.841478323Z" level=info msg="Loading containers: start."
Mar 12 04:13:31 minikube dockerd[1492]: time="2025-03-12T04:13:31.906569444Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.18.0.0/16. Daemon option --bip can be used to set a preferred IP address"
Mar 12 04:13:31 minikube dockerd[1492]: time="2025-03-12T04:13:31.938904378Z" level=info msg="Loading containers: done."
Mar 12 04:13:31 minikube dockerd[1492]: time="2025-03-12T04:13:31.954125034Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
Mar 12 04:13:31 minikube dockerd[1492]: time="2025-03-12T04:13:31.954152979Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
Mar 12 04:13:31 minikube dockerd[1492]: time="2025-03-12T04:13:31.954156482Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
Mar 12 04:13:31 minikube dockerd[1492]: time="2025-03-12T04:13:31.954159064Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
Mar 12 04:13:31 minikube dockerd[1492]: time="2025-03-12T04:13:31.954170141Z" level=info msg="Docker daemon" commit=c710b88 containerd-snapshotter=false storage-driver=overlay2 version=27.4.1
Mar 12 04:13:31 minikube dockerd[1492]: time="2025-03-12T04:13:31.954192238Z" level=info msg="Daemon has completed initialization"
Mar 12 04:13:32 minikube dockerd[1492]: time="2025-03-12T04:13:32.098862404Z" level=info msg="API listen on /var/run/docker.sock"
Mar 12 04:13:32 minikube dockerd[1492]: time="2025-03-12T04:13:32.098876955Z" level=info msg="API listen on [::]:2376"
Mar 12 04:13:32 minikube systemd[1]: Started Docker Application Container Engine.
Mar 12 04:13:32 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
Mar 12 04:13:32 minikube cri-dockerd[1772]: time="2025-03-12T04:13:32Z" level=info msg="Starting cri-dockerd dev (HEAD)"
Mar 12 04:13:32 minikube cri-dockerd[1772]: time="2025-03-12T04:13:32Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
Mar 12 04:13:32 minikube cri-dockerd[1772]: time="2025-03-12T04:13:32Z" level=info msg="Start docker client with request timeout 0s"
Mar 12 04:13:32 minikube cri-dockerd[1772]: time="2025-03-12T04:13:32Z" level=info msg="Hairpin mode is set to hairpin-veth"
Mar 12 04:13:32 minikube cri-dockerd[1772]: time="2025-03-12T04:13:32Z" level=info msg="Loaded network plugin cni"
Mar 12 04:13:32 minikube cri-dockerd[1772]: time="2025-03-12T04:13:32Z" level=info msg="Docker cri networking managed by network plugin cni"
Mar 12 04:13:32 minikube cri-dockerd[1772]: time="2025-03-12T04:13:32Z" level=info msg="Setting cgroupDriver cgroupfs"
Mar 12 04:13:32 minikube cri-dockerd[1772]: time="2025-03-12T04:13:32Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
Mar 12 04:13:32 minikube cri-dockerd[1772]: time="2025-03-12T04:13:32Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
Mar 12 04:13:32 minikube cri-dockerd[1772]: time="2025-03-12T04:13:32Z" level=info msg="Start cri-dockerd grpc backend"
Mar 12 04:13:32 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
Mar 12 04:13:36 minikube cri-dockerd[1772]: time="2025-03-12T04:13:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/d5d73f347db7e3ab7355528cc508139c3d98a852e974ef07288b2df3a0bf148c/resolv.conf as [nameserver 192.168.65.7]"
Mar 12 04:13:36 minikube cri-dockerd[1772]: time="2025-03-12T04:13:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/31f2e97792948f60c4bc779f8c070f7fa34d5719368563ebdda55f18bc75cb42/resolv.conf as [nameserver 192.168.65.7]"
Mar 12 04:13:36 minikube cri-dockerd[1772]: time="2025-03-12T04:13:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/42a4df13e2365197dff45af39014a2e2b6df8d1e63a01bfbb5546c0844d71126/resolv.conf as [nameserver 192.168.65.7]"
Mar 12 04:13:36 minikube cri-dockerd[1772]: time="2025-03-12T04:13:36Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2ee2a29391ff6cc6d69a7953b54de0c5d5066e0b1f93ba3c860e60ca608c6eb3/resolv.conf as [nameserver 192.168.65.7]"
Mar 12 04:13:46 minikube cri-dockerd[1772]: time="2025-03-12T04:13:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/35b5d2647471834580c52573ea3613edbe30f01deb7fe66b40cadbd38cce0da9/resolv.conf as [nameserver 192.168.65.7]"
Mar 12 04:13:46 minikube cri-dockerd[1772]: time="2025-03-12T04:13:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/8346a028d3dfb995d65c8091abf4871f96f6e7a6947569b55a33b51d4baea011/resolv.conf as [nameserver 192.168.65.7]"
Mar 12 04:13:46 minikube cri-dockerd[1772]: time="2025-03-12T04:13:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4fd2af7d2e2d0ddad7385c2dcc5b8f19a4bd2d9c8830aba188c67f62d7786ae8/resolv.conf as [nameserver 192.168.65.7]"
Mar 12 04:13:46 minikube cri-dockerd[1772]: time="2025-03-12T04:13:46Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b7018065b7ee0269d4f31e59461eb077124d8b6e5b2e2c828b6015e8c2be5f01/resolv.conf as [nameserver 192.168.65.7]"
Mar 12 04:13:50 minikube cri-dockerd[1772]: time="2025-03-12T04:13:50Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
d30abbf860ab6       6e38f40d628db       4 minutes ago       Running             storage-provisioner       0                   b7018065b7ee0       storage-provisioner
94eea842c642f       c69fa2e9cbf5f       4 minutes ago       Running             coredns                   0                   4fd2af7d2e2d0       coredns-668d6bf9bc-5kkxq
16477c494de3d       c69fa2e9cbf5f       4 minutes ago       Running             coredns                   0                   8346a028d3dfb       coredns-668d6bf9bc-stqzb
2a65b42f99caa       040f9f8aac8cd       4 minutes ago       Running             kube-proxy                0                   35b5d26474718       kube-proxy-jccbh
efe357ede7a0e       c2e17b8d0f4a3       4 minutes ago       Running             kube-apiserver            0                   2ee2a29391ff6       kube-apiserver-minikube
c3d5af277e92f       a389e107f4ff1       4 minutes ago       Running             kube-scheduler            0                   42a4df13e2365       kube-scheduler-minikube
cb3a85e0c353c       a9e7e6b294baf       4 minutes ago       Running             etcd                      0                   31f2e97792948       etcd-minikube
646664a646c10       8cab3d2a8bd0f       4 minutes ago       Running             kube-controller-manager   0                   d5d73f347db7e       kube-controller-manager-minikube


==> coredns [16477c494de3] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1032989506]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (12-Mar-2025 04:13:46.582) (total time: 21030ms):
Trace[1032989506]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21030ms (04:14:07.611)
Trace[1032989506]: [21.030916856s] [21.030916856s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1818105560]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (12-Mar-2025 04:13:46.582) (total time: 21031ms):
Trace[1818105560]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21030ms (04:14:07.611)
Trace[1818105560]: [21.031039823s] [21.031039823s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[488675648]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (12-Mar-2025 04:13:46.582) (total time: 21031ms):
Trace[488675648]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21030ms (04:14:07.611)
Trace[488675648]: [21.031058241s] [21.031058241s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused


==> coredns [94eea842c642] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 1b226df79860026c6a52e67daa10d7f0d57ec5b023288ec00c5e05f93523c894564e15b91770d3a07ae1cfbe861d15b37d4a0027e69c546ab112970993a3b03b
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[228213868]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (12-Mar-2025 04:13:46.667) (total time: 21040ms):
Trace[228213868]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21040ms (04:14:07.706)
Trace[228213868]: [21.040385488s] [21.040385488s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1958354811]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (12-Mar-2025 04:13:46.667) (total time: 21040ms):
Trace[1958354811]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21040ms (04:14:07.706)
Trace[1958354811]: [21.04049712s] [21.04049712s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[754635148]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (12-Mar-2025 04:13:46.667) (total time: 21040ms):
Trace[754635148]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21040ms (04:14:07.706)
Trace[754635148]: [21.040708035s] [21.040708035s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_03_12T09_43_41_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Wed, 12 Mar 2025 04:13:38 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 12 Mar 2025 04:18:15 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 12 Mar 2025 04:13:50 +0000   Wed, 12 Mar 2025 04:13:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 12 Mar 2025 04:13:50 +0000   Wed, 12 Mar 2025 04:13:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 12 Mar 2025 04:13:50 +0000   Wed, 12 Mar 2025 04:13:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 12 Mar 2025 04:13:50 +0000   Wed, 12 Mar 2025 04:13:38 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  172.17.0.2
  Hostname:    minikube
Capacity:
  cpu:                20
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7975276Ki
  pods:               110
Allocatable:
  cpu:                20
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7975276Ki
  pods:               110
System Info:
  Machine ID:                 7116b331d6b442f485e3101ba99832f1
  System UUID:                7116b331d6b442f485e3101ba99832f1
  Boot ID:                    e8dabc8a-2590-4d2d-9d01-9972afe75cb4
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-668d6bf9bc-5kkxq            100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     4m36s
  kube-system                 coredns-668d6bf9bc-stqzb            100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     4m36s
  kube-system                 etcd-minikube                       100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         4m41s
  kube-system                 kube-apiserver-minikube             250m (1%)     0 (0%)      0 (0%)           0 (0%)         4m41s
  kube-system                 kube-controller-manager-minikube    200m (1%)     0 (0%)      0 (0%)           0 (0%)         4m41s
  kube-system                 kube-proxy-jccbh                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m36s
  kube-system                 kube-scheduler-minikube             100m (0%)     0 (0%)      0 (0%)           0 (0%)         4m42s
  kube-system                 storage-provisioner                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         4m40s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (4%)   0 (0%)
  memory             240Mi (3%)  340Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age    From             Message
  ----     ------                             ----   ----             -------
  Normal   Starting                           4m34s  kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  4m41s  kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           4m41s  kubelet          Starting kubelet.
  Warning  CgroupV1                           4m41s  kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeAllocatableEnforced            4m41s  kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory            4m41s  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              4m41s  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               4m41s  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   RegisteredNode                     4m37s  node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.088697] Aborting journal on device sdd-8.
[  +0.000799] Buffer I/O error on dev sdd, logical block 134184960, lost sync page write
[  +0.001218] JBD2: Error -5 detected when updating journal superblock for sdd-8.
[ +10.668852] FS-Cache: Duplicate cookie detected
[  +0.000465] FS-Cache: O-cookie c=00000046 [p=00000002 fl=222 nc=0 na=1]
[  +0.000584] FS-Cache: O-cookie d=00000000e9492ee3{9P.session} n=000000009c2f2b83
[  +0.000679] FS-Cache: O-key=[10] '34323934393938343635'
[  +0.000517] FS-Cache: N-cookie c=00000047 [p=00000002 fl=2 nc=0 na=1]
[  +0.000796] FS-Cache: N-cookie d=00000000e9492ee3{9P.session} n=000000001abd000e
[  +0.000638] FS-Cache: N-key=[10] '34323934393938343635'
[  +0.006705] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.004382] FS-Cache: Duplicate cookie detected
[  +0.000403] FS-Cache: O-cookie c=00000048 [p=00000002 fl=222 nc=0 na=1]
[  +0.000387] FS-Cache: O-cookie d=00000000e9492ee3{9P.session} n=00000000ad238454
[  +0.000519] FS-Cache: O-key=[10] '34323934393938343636'
[  +0.000486] FS-Cache: N-cookie c=00000049 [p=00000002 fl=2 nc=0 na=1]
[  +0.000365] FS-Cache: N-cookie d=00000000e9492ee3{9P.session} n=00000000d000c04f
[  +0.000668] FS-Cache: N-key=[10] '34323934393938343636'
[  +0.009990] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.105052] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.007680] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001028] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000857] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001371] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.031666] FS-Cache: Duplicate cookie detected
[  +0.000595] FS-Cache: O-cookie c=0000004e [p=00000002 fl=222 nc=0 na=1]
[  +0.000560] FS-Cache: O-cookie d=00000000e9492ee3{9P.session} n=00000000b7668d54
[  +0.000706] FS-Cache: O-key=[10] '34323934393938343832'
[  +0.000566] FS-Cache: N-cookie c=0000004f [p=00000002 fl=2 nc=0 na=1]
[  +0.000528] FS-Cache: N-cookie d=00000000e9492ee3{9P.session} n=00000000d2e02a19
[  +0.000576] FS-Cache: N-key=[10] '34323934393938343832'
[  +0.400900] netlink: 'init': attribute type 4 has an invalid length.
[  +6.009617] blk_update_request: I/O error, dev sde, sector 0 op 0x1:(WRITE) flags 0x800 phys_seg 0 prio class 0
[  +0.054803] Aborting journal on device sde-8.
[  +0.001101] Buffer I/O error on dev sde, logical block 134184960, lost sync page write
[  +0.000618] JBD2: Error -5 detected when updating journal superblock for sde-8.
[  +0.540624] FS-Cache: Duplicate cookie detected
[  +0.000377] FS-Cache: O-cookie c=0000005a [p=00000002 fl=222 nc=0 na=1]
[  +0.000367] FS-Cache: O-cookie d=00000000e9492ee3{9P.session} n=00000000620a8c0e
[  +0.000399] FS-Cache: O-key=[10] '34323934393939313833'
[  +0.000294] FS-Cache: N-cookie c=0000005b [p=00000002 fl=2 nc=0 na=1]
[  +0.000560] FS-Cache: N-cookie d=00000000e9492ee3{9P.session} n=00000000b95779e5
[  +0.000500] FS-Cache: N-key=[10] '34323934393939313833'
[  +0.006327] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.003349] FS-Cache: Duplicate cookie detected
[  +0.000404] FS-Cache: O-cookie c=0000005c [p=00000002 fl=222 nc=0 na=1]
[  +0.000516] FS-Cache: O-cookie d=00000000e9492ee3{9P.session} n=00000000c3d02d67
[  +0.000510] FS-Cache: O-key=[10] '34323934393939313835'
[  +0.000396] FS-Cache: N-cookie c=0000005d [p=00000002 fl=2 nc=0 na=1]
[  +0.000445] FS-Cache: N-cookie d=00000000e9492ee3{9P.session} n=000000000791c12e
[  +0.000553] FS-Cache: N-key=[10] '34323934393939313835'
[  +0.015809] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.052459] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.005300] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000502] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000411] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000494] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.287310] netlink: 'init': attribute type 4 has an invalid length.
[Mar12 04:13] tmpfs: Unknown parameter 'noswap'
[  +5.017113] tmpfs: Unknown parameter 'noswap'


==> etcd [cb3a85e0c353] <==
{"level":"info","ts":"2025-03-12T04:13:36.488932Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://172.17.0.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://172.17.0.2:2380","--initial-cluster=minikube=https://172.17.0.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://172.17.0.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://172.17.0.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-03-12T04:13:36.489428Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-03-12T04:13:36.489460Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://172.17.0.2:2380"]}
{"level":"info","ts":"2025-03-12T04:13:36.489550Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-03-12T04:13:36.489935Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://172.17.0.2:2379"]}
{"level":"info","ts":"2025-03-12T04:13:36.490011Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":20,"max-cpu-available":20,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://172.17.0.2:2380"],"listen-peer-urls":["https://172.17.0.2:2380"],"advertise-client-urls":["https://172.17.0.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://172.17.0.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://172.17.0.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-03-12T04:13:36.544609Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"54.210459ms"}
{"level":"info","ts":"2025-03-12T04:13:36.554915Z","caller":"etcdserver/raft.go:505","msg":"starting local member","local-member-id":"b8e14bda2255bc24","cluster-id":"38b0e74a458e7a1f"}
{"level":"info","ts":"2025-03-12T04:13:36.555203Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 switched to configuration voters=()"}
{"level":"info","ts":"2025-03-12T04:13:36.555516Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 became follower at term 0"}
{"level":"info","ts":"2025-03-12T04:13:36.555567Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft b8e14bda2255bc24 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-03-12T04:13:36.555579Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 became follower at term 1"}
{"level":"info","ts":"2025-03-12T04:13:36.555689Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 switched to configuration voters=(13322012572989635620)"}
{"level":"warn","ts":"2025-03-12T04:13:36.565145Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-03-12T04:13:36.567328Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-03-12T04:13:36.569467Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-03-12T04:13:36.572953Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"b8e14bda2255bc24","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-03-12T04:13:36.573155Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"b8e14bda2255bc24","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-03-12T04:13:36.573446Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-12T04:13:36.573380Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-12T04:13:36.573606Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-12T04:13:36.573616Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-03-12T04:13:36.576319Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-03-12T04:13:36.576741Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"b8e14bda2255bc24","initial-advertise-peer-urls":["https://172.17.0.2:2380"],"listen-peer-urls":["https://172.17.0.2:2380"],"advertise-client-urls":["https://172.17.0.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://172.17.0.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-03-12T04:13:36.576781Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-03-12T04:13:36.576840Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"172.17.0.2:2380"}
{"level":"info","ts":"2025-03-12T04:13:36.576846Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"172.17.0.2:2380"}
{"level":"info","ts":"2025-03-12T04:13:36.644600Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 switched to configuration voters=(13322012572989635620)"}
{"level":"info","ts":"2025-03-12T04:13:36.645006Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"38b0e74a458e7a1f","local-member-id":"b8e14bda2255bc24","added-peer-id":"b8e14bda2255bc24","added-peer-peer-urls":["https://172.17.0.2:2380"]}
{"level":"info","ts":"2025-03-12T04:13:36.857103Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 is starting a new election at term 1"}
{"level":"info","ts":"2025-03-12T04:13:36.857144Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 became pre-candidate at term 1"}
{"level":"info","ts":"2025-03-12T04:13:36.857155Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 received MsgPreVoteResp from b8e14bda2255bc24 at term 1"}
{"level":"info","ts":"2025-03-12T04:13:36.857166Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 became candidate at term 2"}
{"level":"info","ts":"2025-03-12T04:13:36.857170Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 received MsgVoteResp from b8e14bda2255bc24 at term 2"}
{"level":"info","ts":"2025-03-12T04:13:36.857176Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"b8e14bda2255bc24 became leader at term 2"}
{"level":"info","ts":"2025-03-12T04:13:36.857180Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: b8e14bda2255bc24 elected leader b8e14bda2255bc24 at term 2"}
{"level":"info","ts":"2025-03-12T04:13:36.858580Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"b8e14bda2255bc24","local-member-attributes":"{Name:minikube ClientURLs:[https://172.17.0.2:2379]}","request-path":"/0/members/b8e14bda2255bc24/attributes","cluster-id":"38b0e74a458e7a1f","publish-timeout":"7s"}
{"level":"info","ts":"2025-03-12T04:13:36.858734Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-12T04:13:36.858800Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-12T04:13:36.858915Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-03-12T04:13:36.859111Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-03-12T04:13:36.859137Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-03-12T04:13:36.859369Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-12T04:13:36.859474Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-03-12T04:13:36.859730Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"172.17.0.2:2379"}
{"level":"info","ts":"2025-03-12T04:13:36.859798Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"38b0e74a458e7a1f","local-member-id":"b8e14bda2255bc24","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-12T04:13:36.859838Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-12T04:13:36.859849Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-03-12T04:13:36.860045Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-03-12T04:13:38.451762Z","caller":"traceutil/trace.go:171","msg":"trace[184699510] transaction","detail":"{read_only:false; response_revision:4; number_of_response:1; }","duration":"101.932002ms","start":"2025-03-12T04:13:38.349798Z","end":"2025-03-12T04:13:38.451730Z","steps":["trace[184699510] 'process raft request'  (duration: 101.726013ms)"],"step_count":1}
{"level":"info","ts":"2025-03-12T04:13:38.451793Z","caller":"traceutil/trace.go:171","msg":"trace[1579184266] transaction","detail":"{read_only:false; response_revision:3; number_of_response:1; }","duration":"102.116513ms","start":"2025-03-12T04:13:38.349652Z","end":"2025-03-12T04:13:38.451769Z","steps":["trace[1579184266] 'process raft request'  (duration: 101.708708ms)"],"step_count":1}
{"level":"info","ts":"2025-03-12T04:13:38.451950Z","caller":"traceutil/trace.go:171","msg":"trace[1530163603] linearizableReadLoop","detail":"{readStateIndex:7; appliedIndex:4; }","duration":"101.136096ms","start":"2025-03-12T04:13:38.350798Z","end":"2025-03-12T04:13:38.451934Z","steps":["trace[1530163603] 'read index received'  (duration: 95.976841ms)","trace[1530163603] 'applied index is now lower than readState.Index'  (duration: 5.158773ms)"],"step_count":2}
{"level":"warn","ts":"2025-03-12T04:13:38.452138Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.288365ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/minikube\" limit:1 ","response":"range_response_count:1 size:2874"}
{"level":"info","ts":"2025-03-12T04:13:38.452174Z","caller":"traceutil/trace.go:171","msg":"trace[897644816] range","detail":"{range_begin:/registry/minions/minikube; range_end:; response_count:1; response_revision:11; }","duration":"101.34814ms","start":"2025-03-12T04:13:38.350814Z","end":"2025-03-12T04:13:38.452163Z","steps":["trace[897644816] 'agreement among raft nodes before linearized reading'  (duration: 101.242204ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-12T04:13:38.452170Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"101.366593ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/minions/minikube\" limit:1 ","response":"range_response_count:1 size:2874"}
{"level":"info","ts":"2025-03-12T04:13:38.452238Z","caller":"traceutil/trace.go:171","msg":"trace[1014103291] range","detail":"{range_begin:/registry/minions/minikube; range_end:; response_count:1; response_revision:11; }","duration":"101.449868ms","start":"2025-03-12T04:13:38.350782Z","end":"2025-03-12T04:13:38.452232Z","steps":["trace[1014103291] 'agreement among raft nodes before linearized reading'  (duration: 101.359686ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-12T04:13:38.452193Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.497259ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/namespaces/kube-system\" limit:1 ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-03-12T04:13:38.452280Z","caller":"traceutil/trace.go:171","msg":"trace[25436170] range","detail":"{range_begin:/registry/namespaces/kube-system; range_end:; response_count:0; response_revision:11; }","duration":"100.579371ms","start":"2025-03-12T04:13:38.351679Z","end":"2025-03-12T04:13:38.452258Z","steps":["trace[25436170] 'agreement among raft nodes before linearized reading'  (duration: 100.491558ms)"],"step_count":1}
{"level":"warn","ts":"2025-03-12T04:13:38.455774Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.479221ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:4"}
{"level":"info","ts":"2025-03-12T04:13:38.455841Z","caller":"traceutil/trace.go:171","msg":"trace[125320065] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:11; }","duration":"104.567023ms","start":"2025-03-12T04:13:38.351257Z","end":"2025-03-12T04:13:38.455824Z","steps":["trace[125320065] 'agreement among raft nodes before linearized reading'  (duration: 100.943701ms)"],"step_count":1}


==> kernel <==
 04:18:21 up 22 min,  0 users,  load average: 0.13, 0.13, 0.08
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [efe357ede7a0] <==
I0312 04:13:38.178356       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0312 04:13:38.178372       1 controller.go:119] Starting legacy_token_tracking_controller
I0312 04:13:38.178384       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0312 04:13:38.178415       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0312 04:13:38.178585       1 apf_controller.go:377] Starting API Priority and Fairness config controller
I0312 04:13:38.178601       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0312 04:13:38.178608       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0312 04:13:38.178612       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0312 04:13:38.178686       1 apiservice_controller.go:100] Starting APIServiceRegistrationController
I0312 04:13:38.178707       1 controller.go:142] Starting OpenAPI controller
I0312 04:13:38.178708       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0312 04:13:38.178718       1 controller.go:90] Starting OpenAPI V3 controller
I0312 04:13:38.178725       1 naming_controller.go:294] Starting NamingConditionController
I0312 04:13:38.178735       1 establishing_controller.go:81] Starting EstablishingController
I0312 04:13:38.178749       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0312 04:13:38.178936       1 controller.go:78] Starting OpenAPI AggregationController
I0312 04:13:38.178941       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0312 04:13:38.178958       1 crd_finalizer.go:269] Starting CRDFinalizer
I0312 04:13:38.178742       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0312 04:13:38.178947       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0312 04:13:38.178695       1 customresource_discovery_controller.go:292] Starting DiscoveryController
I0312 04:13:38.214796       1 cluster_authentication_trust_controller.go:462] Starting cluster_authentication_trust_controller controller
I0312 04:13:38.214887       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0312 04:13:38.214909       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0312 04:13:38.214920       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0312 04:13:38.344998       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0312 04:13:38.345292       1 policy_source.go:240] refreshing policies
I0312 04:13:38.345191       1 cache.go:39] Caches are synced for LocalAvailability controller
I0312 04:13:38.345610       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0312 04:13:38.345988       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0312 04:13:38.346148       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0312 04:13:38.346300       1 shared_informer.go:320] Caches are synced for configmaps
I0312 04:13:38.346989       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0312 04:13:38.347047       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0312 04:13:38.347426       1 aggregator.go:171] initial CRD sync complete...
I0312 04:13:38.347440       1 autoregister_controller.go:144] Starting autoregister controller
I0312 04:13:38.347446       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0312 04:13:38.347452       1 cache.go:39] Caches are synced for autoregister controller
I0312 04:13:38.348892       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0312 04:13:38.349046       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
E0312 04:13:38.350617       1 controller.go:145] "Failed to ensure lease exists, will retry" err="namespaces \"kube-system\" not found" interval="200ms"
I0312 04:13:38.352456       1 controller.go:615] quota admission added evaluator for: namespaces
I0312 04:13:38.444793       1 shared_informer.go:320] Caches are synced for node_authorizer
E0312 04:13:38.453734       1 controller.go:148] "Unhandled Error" err="while syncing ConfigMap \"kube-system/kube-apiserver-legacy-service-account-token-tracking\", err: namespaces \"kube-system\" not found" logger="UnhandledError"
I0312 04:13:38.554319       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0312 04:13:39.182758       1 storage_scheduling.go:95] created PriorityClass system-node-critical with value 2000001000
I0312 04:13:39.185627       1 storage_scheduling.go:95] created PriorityClass system-cluster-critical with value 2000000000
I0312 04:13:39.185654       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0312 04:13:39.563964       1 controller.go:615] quota admission added evaluator for: roles.rbac.authorization.k8s.io
I0312 04:13:39.596421       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0312 04:13:39.654467       1 alloc.go:330] "allocated clusterIPs" service="default/kubernetes" clusterIPs={"IPv4":"10.96.0.1"}
W0312 04:13:39.659248       1 lease.go:265] Resetting endpoints for master service "kubernetes" to [172.17.0.2]
I0312 04:13:39.659824       1 controller.go:615] quota admission added evaluator for: endpoints
I0312 04:13:39.663197       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0312 04:13:40.263839       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0312 04:13:40.474093       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0312 04:13:40.558761       1 alloc.go:330] "allocated clusterIPs" service="kube-system/kube-dns" clusterIPs={"IPv4":"10.96.0.10"}
I0312 04:13:40.567071       1 controller.go:615] quota admission added evaluator for: daemonsets.apps
I0312 04:13:45.565752       1 controller.go:615] quota admission added evaluator for: controllerrevisions.apps
I0312 04:13:45.814631       1 controller.go:615] quota admission added evaluator for: replicasets.apps


==> kube-controller-manager [646664a646c1] <==
I0312 04:13:44.763525       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0312 04:13:44.763540       1 shared_informer.go:320] Caches are synced for disruption
I0312 04:13:44.765012       1 shared_informer.go:320] Caches are synced for PV protection
I0312 04:13:44.765031       1 shared_informer.go:320] Caches are synced for cronjob
I0312 04:13:44.765057       1 shared_informer.go:320] Caches are synced for endpoint
I0312 04:13:44.765075       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0312 04:13:44.765216       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0312 04:13:44.765258       1 shared_informer.go:320] Caches are synced for TTL
I0312 04:13:44.765292       1 shared_informer.go:320] Caches are synced for service account
I0312 04:13:44.765339       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0312 04:13:44.765493       1 shared_informer.go:320] Caches are synced for ReplicationController
I0312 04:13:44.765539       1 shared_informer.go:320] Caches are synced for node
I0312 04:13:44.765662       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0312 04:13:44.765706       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0312 04:13:44.765711       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0312 04:13:44.765714       1 shared_informer.go:320] Caches are synced for cidrallocator
I0312 04:13:44.766086       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0312 04:13:44.770270       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0312 04:13:44.770331       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0312 04:13:44.770364       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0312 04:13:44.771305       1 shared_informer.go:320] Caches are synced for daemon sets
I0312 04:13:44.779554       1 shared_informer.go:320] Caches are synced for HPA
I0312 04:13:44.788840       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0312 04:13:44.794166       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0312 04:13:44.799461       1 shared_informer.go:320] Caches are synced for ephemeral
I0312 04:13:44.810965       1 shared_informer.go:320] Caches are synced for taint
I0312 04:13:44.811022       1 node_lifecycle_controller.go:1234] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0312 04:13:44.811063       1 node_lifecycle_controller.go:886] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0312 04:13:44.811113       1 node_lifecycle_controller.go:1080] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0312 04:13:44.814467       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0312 04:13:44.814520       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0312 04:13:44.814533       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0312 04:13:44.814575       1 shared_informer.go:320] Caches are synced for job
I0312 04:13:44.814582       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0312 04:13:44.814575       1 shared_informer.go:320] Caches are synced for expand
I0312 04:13:44.814618       1 shared_informer.go:320] Caches are synced for crt configmap
I0312 04:13:44.814663       1 shared_informer.go:320] Caches are synced for PVC protection
I0312 04:13:44.814664       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0312 04:13:44.814739       1 shared_informer.go:320] Caches are synced for deployment
I0312 04:13:44.814673       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0312 04:13:44.816006       1 shared_informer.go:320] Caches are synced for TTL after finished
I0312 04:13:44.816034       1 shared_informer.go:320] Caches are synced for GC
I0312 04:13:44.817124       1 shared_informer.go:320] Caches are synced for namespace
I0312 04:13:44.818331       1 shared_informer.go:320] Caches are synced for attach detach
I0312 04:13:44.818705       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0312 04:13:44.820917       1 shared_informer.go:320] Caches are synced for resource quota
I0312 04:13:44.823526       1 shared_informer.go:320] Caches are synced for resource quota
I0312 04:13:44.829707       1 shared_informer.go:320] Caches are synced for garbage collector
I0312 04:13:45.520943       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0312 04:13:45.954031       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="135.146341ms"
I0312 04:13:45.963908       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="9.795391ms"
I0312 04:13:45.964076       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="74.634µs"
I0312 04:13:45.973882       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="55.879µs"
I0312 04:13:47.593847       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="66.962µs"
I0312 04:13:47.615547       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="46.45µs"
I0312 04:13:50.768001       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0312 04:14:09.613731       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="6.492572ms"
I0312 04:14:09.613883       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="72.273µs"
I0312 04:14:10.678201       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="6.789431ms"
I0312 04:14:10.678302       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="51.04µs"


==> kube-proxy [2a65b42f99ca] <==
I0312 04:13:46.250313       1 server_linux.go:66] "Using iptables proxy"
I0312 04:13:46.489616       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["172.17.0.2"]
E0312 04:13:46.489678       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0312 04:13:46.504629       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0312 04:13:46.504718       1 server_linux.go:170] "Using iptables Proxier"
I0312 04:13:46.506130       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0312 04:13:46.512893       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0312 04:13:46.518311       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0312 04:13:46.518378       1 server.go:497] "Version info" version="v1.32.0"
I0312 04:13:46.518397       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0312 04:13:46.524258       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0312 04:13:46.528590       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0312 04:13:46.529258       1 config.go:199] "Starting service config controller"
I0312 04:13:46.529278       1 shared_informer.go:313] Waiting for caches to sync for service config
I0312 04:13:46.529303       1 config.go:105] "Starting endpoint slice config controller"
I0312 04:13:46.529305       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0312 04:13:46.529320       1 config.go:329] "Starting node config controller"
I0312 04:13:46.529341       1 shared_informer.go:313] Waiting for caches to sync for node config
I0312 04:13:46.630077       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0312 04:13:46.630104       1 shared_informer.go:320] Caches are synced for node config
I0312 04:13:46.630090       1 shared_informer.go:320] Caches are synced for service config


==> kube-scheduler [c3d5af277e92] <==
I0312 04:13:37.261394       1 serving.go:386] Generated self-signed cert in-memory
W0312 04:13:38.255939       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0312 04:13:38.256041       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0312 04:13:38.256058       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0312 04:13:38.256066       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0312 04:13:38.449593       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0312 04:13:38.449620       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0312 04:13:38.450684       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0312 04:13:38.450722       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0312 04:13:38.450732       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0312 04:13:38.450723       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
W0312 04:13:38.451929       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0312 04:13:38.452016       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0312 04:13:38.452178       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0312 04:13:38.452200       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 04:13:38.452236       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
W0312 04:13:38.452241       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0312 04:13:38.452253       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
E0312 04:13:38.452276       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0312 04:13:38.452327       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0312 04:13:38.452339       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0312 04:13:38.452401       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0312 04:13:38.452428       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0312 04:13:38.452655       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0312 04:13:38.452690       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 04:13:38.452755       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0312 04:13:38.452771       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 04:13:38.452854       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0312 04:13:38.452883       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 04:13:38.453050       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E0312 04:13:38.453077       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 04:13:38.453141       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0312 04:13:38.453183       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0312 04:13:38.453296       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0312 04:13:38.453315       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0312 04:13:38.451933       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0312 04:13:38.453345       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0312 04:13:38.453416       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
E0312 04:13:38.453351       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 04:13:38.455481       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0312 04:13:38.455528       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0312 04:13:38.455481       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0312 04:13:38.455558       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0312 04:13:39.255677       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0312 04:13:39.255721       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0312 04:13:39.408205       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0312 04:13:39.408252       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0312 04:13:39.422296       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0312 04:13:39.422333       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0312 04:13:39.427263       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0312 04:13:39.427297       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
I0312 04:13:40.051826       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kubelet <==
Mar 12 04:13:40 minikube kubelet[2653]: E0312 04:13:40.545341    2653 eviction_manager.go:267] "eviction manager: failed to check if we have separate container filesystem. Ignoring." err="no imagefs label for configured runtime"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.553416    2653 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.553551    2653 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.553627    2653 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-controller-manager-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.553416    2653 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: E0312 04:13:40.566009    2653 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.580759    2653 kubelet_node_status.go:76] "Attempting to register node" node="minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.588526    2653 kubelet_node_status.go:125] "Node was previously registered" node="minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.588831    2653 kubelet_node_status.go:79] "Successfully registered node" node="minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.645398    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-usr-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.645431    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-certs\" (UniqueName: \"kubernetes.io/host-path/a8ad368138d50569985de17360b120c4-etcd-certs\") pod \"etcd-minikube\" (UID: \"a8ad368138d50569985de17360b120c4\") " pod="kube-system/etcd-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.645445    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/24b8ed0803f663dbd470e51cd458b097-usr-local-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"24b8ed0803f663dbd470e51cd458b097\") " pod="kube-system/kube-apiserver-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.645453    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/24b8ed0803f663dbd470e51cd458b097-usr-share-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"24b8ed0803f663dbd470e51cd458b097\") " pod="kube-system/kube-apiserver-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.645460    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-etc-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.645466    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-k8s-certs\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.645472    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"usr-local-share-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-usr-local-share-ca-certificates\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.645479    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-ca-certs\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.645487    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"flexvolume-dir\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-flexvolume-dir\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.645493    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etcd-data\" (UniqueName: \"kubernetes.io/host-path/a8ad368138d50569985de17360b120c4-etcd-data\") pod \"etcd-minikube\" (UID: \"a8ad368138d50569985de17360b120c4\") " pod="kube-system/etcd-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.645500    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"ca-certs\" (UniqueName: \"kubernetes.io/host-path/24b8ed0803f663dbd470e51cd458b097-ca-certs\") pod \"kube-apiserver-minikube\" (UID: \"24b8ed0803f663dbd470e51cd458b097\") " pod="kube-system/kube-apiserver-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.645510    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"etc-ca-certificates\" (UniqueName: \"kubernetes.io/host-path/24b8ed0803f663dbd470e51cd458b097-etc-ca-certificates\") pod \"kube-apiserver-minikube\" (UID: \"24b8ed0803f663dbd470e51cd458b097\") " pod="kube-system/kube-apiserver-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.645518    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"k8s-certs\" (UniqueName: \"kubernetes.io/host-path/24b8ed0803f663dbd470e51cd458b097-k8s-certs\") pod \"kube-apiserver-minikube\" (UID: \"24b8ed0803f663dbd470e51cd458b097\") " pod="kube-system/kube-apiserver-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.645524    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/843c74f7b3bc7d7040a05c31708a6a30-kubeconfig\") pod \"kube-controller-manager-minikube\" (UID: \"843c74f7b3bc7d7040a05c31708a6a30\") " pod="kube-system/kube-controller-manager-minikube"
Mar 12 04:13:40 minikube kubelet[2653]: I0312 04:13:40.645553    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kubeconfig\" (UniqueName: \"kubernetes.io/host-path/d14ce008bee3a1f3bd7cf547688f9dfe-kubeconfig\") pod \"kube-scheduler-minikube\" (UID: \"d14ce008bee3a1f3bd7cf547688f9dfe\") " pod="kube-system/kube-scheduler-minikube"
Mar 12 04:13:41 minikube kubelet[2653]: I0312 04:13:41.432776    2653 apiserver.go:52] "Watching apiserver"
Mar 12 04:13:41 minikube kubelet[2653]: I0312 04:13:41.444100    2653 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
Mar 12 04:13:41 minikube kubelet[2653]: I0312 04:13:41.478489    2653 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/etcd-minikube"
Mar 12 04:13:41 minikube kubelet[2653]: I0312 04:13:41.478579    2653 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-apiserver-minikube"
Mar 12 04:13:41 minikube kubelet[2653]: I0312 04:13:41.478709    2653 kubelet.go:3200] "Creating a mirror pod for static pod" pod="kube-system/kube-scheduler-minikube"
Mar 12 04:13:41 minikube kubelet[2653]: E0312 04:13:41.490048    2653 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"etcd-minikube\" already exists" pod="kube-system/etcd-minikube"
Mar 12 04:13:41 minikube kubelet[2653]: E0312 04:13:41.490577    2653 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-scheduler-minikube\" already exists" pod="kube-system/kube-scheduler-minikube"
Mar 12 04:13:41 minikube kubelet[2653]: E0312 04:13:41.490701    2653 kubelet.go:3202] "Failed creating a mirror pod" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
Mar 12 04:13:41 minikube kubelet[2653]: I0312 04:13:41.651728    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-apiserver-minikube" podStartSLOduration=1.651708344 podStartE2EDuration="1.651708344s" podCreationTimestamp="2025-03-12 04:13:40 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-12 04:13:41.560674243 +0000 UTC m=+1.191485176" watchObservedRunningTime="2025-03-12 04:13:41.651708344 +0000 UTC m=+1.282519264"
Mar 12 04:13:41 minikube kubelet[2653]: I0312 04:13:41.651811    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-controller-manager-minikube" podStartSLOduration=1.6518076019999999 podStartE2EDuration="1.651807602s" podCreationTimestamp="2025-03-12 04:13:40 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-12 04:13:41.651702821 +0000 UTC m=+1.282513740" watchObservedRunningTime="2025-03-12 04:13:41.651807602 +0000 UTC m=+1.282618523"
Mar 12 04:13:41 minikube kubelet[2653]: I0312 04:13:41.664664    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-scheduler-minikube" podStartSLOduration=2.664650493 podStartE2EDuration="2.664650493s" podCreationTimestamp="2025-03-12 04:13:39 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-12 04:13:41.664598776 +0000 UTC m=+1.295409710" watchObservedRunningTime="2025-03-12 04:13:41.664650493 +0000 UTC m=+1.295461412"
Mar 12 04:13:41 minikube kubelet[2653]: I0312 04:13:41.664823    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/etcd-minikube" podStartSLOduration=1.664806598 podStartE2EDuration="1.664806598s" podCreationTimestamp="2025-03-12 04:13:40 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-12 04:13:41.657380006 +0000 UTC m=+1.288190922" watchObservedRunningTime="2025-03-12 04:13:41.664806598 +0000 UTC m=+1.295617514"
Mar 12 04:13:44 minikube kubelet[2653]: I0312 04:13:44.873897    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-grslm\" (UniqueName: \"kubernetes.io/projected/9c4cc989-6680-4043-a343-a7ad29921027-kube-api-access-grslm\") pod \"storage-provisioner\" (UID: \"9c4cc989-6680-4043-a343-a7ad29921027\") " pod="kube-system/storage-provisioner"
Mar 12 04:13:44 minikube kubelet[2653]: I0312 04:13:44.873936    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/9c4cc989-6680-4043-a343-a7ad29921027-tmp\") pod \"storage-provisioner\" (UID: \"9c4cc989-6680-4043-a343-a7ad29921027\") " pod="kube-system/storage-provisioner"
Mar 12 04:13:44 minikube kubelet[2653]: E0312 04:13:44.978545    2653 projected.go:288] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Mar 12 04:13:44 minikube kubelet[2653]: E0312 04:13:44.978576    2653 projected.go:194] Error preparing data for projected volume kube-api-access-grslm for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Mar 12 04:13:44 minikube kubelet[2653]: E0312 04:13:44.978621    2653 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/9c4cc989-6680-4043-a343-a7ad29921027-kube-api-access-grslm podName:9c4cc989-6680-4043-a343-a7ad29921027 nodeName:}" failed. No retries permitted until 2025-03-12 04:13:45.47860787 +0000 UTC m=+5.109418781 (durationBeforeRetry 500ms). Error: MountVolume.SetUp failed for volume "kube-api-access-grslm" (UniqueName: "kubernetes.io/projected/9c4cc989-6680-4043-a343-a7ad29921027-kube-api-access-grslm") pod "storage-provisioner" (UID: "9c4cc989-6680-4043-a343-a7ad29921027") : configmap "kube-root-ca.crt" not found
Mar 12 04:13:45 minikube kubelet[2653]: E0312 04:13:45.578556    2653 projected.go:288] Couldn't get configMap kube-system/kube-root-ca.crt: configmap "kube-root-ca.crt" not found
Mar 12 04:13:45 minikube kubelet[2653]: E0312 04:13:45.578593    2653 projected.go:194] Error preparing data for projected volume kube-api-access-grslm for pod kube-system/storage-provisioner: configmap "kube-root-ca.crt" not found
Mar 12 04:13:45 minikube kubelet[2653]: E0312 04:13:45.578642    2653 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/projected/9c4cc989-6680-4043-a343-a7ad29921027-kube-api-access-grslm podName:9c4cc989-6680-4043-a343-a7ad29921027 nodeName:}" failed. No retries permitted until 2025-03-12 04:13:46.578631353 +0000 UTC m=+6.209442270 (durationBeforeRetry 1s). Error: MountVolume.SetUp failed for volume "kube-api-access-grslm" (UniqueName: "kubernetes.io/projected/9c4cc989-6680-4043-a343-a7ad29921027-kube-api-access-grslm") pod "storage-provisioner" (UID: "9c4cc989-6680-4043-a343-a7ad29921027") : configmap "kube-root-ca.crt" not found
Mar 12 04:13:45 minikube kubelet[2653]: I0312 04:13:45.678671    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-proxy\" (UniqueName: \"kubernetes.io/configmap/05fdf683-1d3a-4f3c-b98a-c63655091f76-kube-proxy\") pod \"kube-proxy-jccbh\" (UID: \"05fdf683-1d3a-4f3c-b98a-c63655091f76\") " pod="kube-system/kube-proxy-jccbh"
Mar 12 04:13:45 minikube kubelet[2653]: I0312 04:13:45.678711    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/05fdf683-1d3a-4f3c-b98a-c63655091f76-lib-modules\") pod \"kube-proxy-jccbh\" (UID: \"05fdf683-1d3a-4f3c-b98a-c63655091f76\") " pod="kube-system/kube-proxy-jccbh"
Mar 12 04:13:45 minikube kubelet[2653]: I0312 04:13:45.678719    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/05fdf683-1d3a-4f3c-b98a-c63655091f76-xtables-lock\") pod \"kube-proxy-jccbh\" (UID: \"05fdf683-1d3a-4f3c-b98a-c63655091f76\") " pod="kube-system/kube-proxy-jccbh"
Mar 12 04:13:45 minikube kubelet[2653]: I0312 04:13:45.678726    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-45hbr\" (UniqueName: \"kubernetes.io/projected/05fdf683-1d3a-4f3c-b98a-c63655091f76-kube-api-access-45hbr\") pod \"kube-proxy-jccbh\" (UID: \"05fdf683-1d3a-4f3c-b98a-c63655091f76\") " pod="kube-system/kube-proxy-jccbh"
Mar 12 04:13:45 minikube kubelet[2653]: I0312 04:13:45.980149    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-58m7p\" (UniqueName: \"kubernetes.io/projected/5c28375b-2aea-46c8-bc40-74609750a877-kube-api-access-58m7p\") pod \"coredns-668d6bf9bc-stqzb\" (UID: \"5c28375b-2aea-46c8-bc40-74609750a877\") " pod="kube-system/coredns-668d6bf9bc-stqzb"
Mar 12 04:13:45 minikube kubelet[2653]: I0312 04:13:45.980183    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-vwvbw\" (UniqueName: \"kubernetes.io/projected/10bf4a97-de24-483f-b81a-3bd461432d10-kube-api-access-vwvbw\") pod \"coredns-668d6bf9bc-5kkxq\" (UID: \"10bf4a97-de24-483f-b81a-3bd461432d10\") " pod="kube-system/coredns-668d6bf9bc-5kkxq"
Mar 12 04:13:45 minikube kubelet[2653]: I0312 04:13:45.980190    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/5c28375b-2aea-46c8-bc40-74609750a877-config-volume\") pod \"coredns-668d6bf9bc-stqzb\" (UID: \"5c28375b-2aea-46c8-bc40-74609750a877\") " pod="kube-system/coredns-668d6bf9bc-stqzb"
Mar 12 04:13:45 minikube kubelet[2653]: I0312 04:13:45.980199    2653 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"config-volume\" (UniqueName: \"kubernetes.io/configmap/10bf4a97-de24-483f-b81a-3bd461432d10-config-volume\") pod \"coredns-668d6bf9bc-5kkxq\" (UID: \"10bf4a97-de24-483f-b81a-3bd461432d10\") " pod="kube-system/coredns-668d6bf9bc-5kkxq"
Mar 12 04:13:46 minikube kubelet[2653]: I0312 04:13:46.599905    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/kube-proxy-jccbh" podStartSLOduration=1.599891361 podStartE2EDuration="1.599891361s" podCreationTimestamp="2025-03-12 04:13:45 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-12 04:13:46.599722171 +0000 UTC m=+6.230533105" watchObservedRunningTime="2025-03-12 04:13:46.599891361 +0000 UTC m=+6.230702280"
Mar 12 04:13:47 minikube kubelet[2653]: I0312 04:13:47.601436    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-668d6bf9bc-5kkxq" podStartSLOduration=2.601382097 podStartE2EDuration="2.601382097s" podCreationTimestamp="2025-03-12 04:13:45 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-12 04:13:47.593546153 +0000 UTC m=+7.224357070" watchObservedRunningTime="2025-03-12 04:13:47.601382097 +0000 UTC m=+7.232193016"
Mar 12 04:13:47 minikube kubelet[2653]: I0312 04:13:47.608742    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/storage-provisioner" podStartSLOduration=6.608719702 podStartE2EDuration="6.608719702s" podCreationTimestamp="2025-03-12 04:13:41 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-12 04:13:47.608567947 +0000 UTC m=+7.239378872" watchObservedRunningTime="2025-03-12 04:13:47.608719702 +0000 UTC m=+7.239530618"
Mar 12 04:13:47 minikube kubelet[2653]: I0312 04:13:47.615557    2653 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="kube-system/coredns-668d6bf9bc-stqzb" podStartSLOduration=2.615541139 podStartE2EDuration="2.615541139s" podCreationTimestamp="2025-03-12 04:13:45 +0000 UTC" firstStartedPulling="0001-01-01 00:00:00 +0000 UTC" lastFinishedPulling="0001-01-01 00:00:00 +0000 UTC" observedRunningTime="2025-03-12 04:13:47.61536404 +0000 UTC m=+7.246174958" watchObservedRunningTime="2025-03-12 04:13:47.615541139 +0000 UTC m=+7.246352059"
Mar 12 04:13:48 minikube kubelet[2653]: I0312 04:13:48.596458    2653 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Mar 12 04:13:50 minikube kubelet[2653]: I0312 04:13:50.658633    2653 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
Mar 12 04:13:50 minikube kubelet[2653]: I0312 04:13:50.760729    2653 kuberuntime_manager.go:1702] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
Mar 12 04:13:50 minikube kubelet[2653]: I0312 04:13:50.761347    2653 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"


==> storage-provisioner [d30abbf860ab] <==
I0312 04:13:46.918659       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0312 04:13:46.923867       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0312 04:13:46.923898       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0312 04:13:46.928380       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0312 04:13:46.928491       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_9d9f8a84-8fce-407d-8080-bcf0b7195f8a!
I0312 04:13:46.928491       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"5064fd2e-022a-40ec-a90e-07d21db806b0", APIVersion:"v1", ResourceVersion:"364", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_9d9f8a84-8fce-407d-8080-bcf0b7195f8a became leader
I0312 04:13:47.028589       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_9d9f8a84-8fce-407d-8080-bcf0b7195f8a!

